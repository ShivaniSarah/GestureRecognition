{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gesture Recognition\n",
    "In this group project, you are going to build a 3D Conv model that will be able to predict the 5 gestures correctly. Please import the following libraries to get started."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.tuna.tsinghua.edu.cn/simple\n",
      "Collecting scipy==1.1.0\n",
      "  Using cached scipy-1.1.0-cp38-cp38-linux_x86_64.whl\n",
      "Requirement already satisfied: numpy>=1.8.2 in /usr/local/lib/python3.8/dist-packages (from scipy==1.1.0) (1.19.4)\n",
      "Installing collected packages: scipy\n",
      "  Attempting uninstall: scipy\n",
      "    Found existing installation: scipy 1.4.1\n",
      "    Uninstalling scipy-1.4.1:\n",
      "      Successfully uninstalled scipy-1.4.1\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "scikit-image 0.19.2 requires scipy>=1.4.1, but you have scipy 1.1.0 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed scipy-1.1.0\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: You are using pip version 22.0.3; however, version 22.0.4 is available.\n",
      "You should consider upgrading via the '/usr/bin/python -m pip install --upgrade pip' command.\u001b[0m\u001b[33m\n",
      "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "#pip install scipy==1.1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "from scipy.misc import imread, imresize\n",
    "import datetime\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We set the random seed so that the results don't vary drastically."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(30)\n",
    "import random as rn\n",
    "rn.seed(30)\n",
    "from keras import backend as K\n",
    "import tensorflow as tf\n",
    "tf.random.set_seed(30)\n",
    "#tf.set_random_seed(30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this block, you read the folder names for training and validation. You also set the `batch_size` here. Note that you set the batch size in such a way that you are able to use the GPU in full capacity. You keep increasing the batch size until the machine throws an error."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Learning Course Project - Gesture Recognition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem Statement"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Imagine you are working as a data scientist at a home electronics company which manufactures state of the art smart televisions. You want to develop a cool feature in the smart-TV that can recognise five different gestures performed by the user which will help users control the TV without using a remote.\n",
    "\n",
    "The gestures are continuously monitored by the webcam mounted on the TV. Each gesture corresponds to a specific command:\n",
    "\n",
    "Gesture\tCorresponding Action\n",
    "Thumbs Up\tIncrease the volume.\n",
    "Thumbs Down\tDecrease the volume.\n",
    "Left Swipe\t'Jump' backwards 10 seconds.\n",
    "Right Swipe\t'Jump' forward 10 seconds.\n",
    "Stop\tPause the movie."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Objectives:\n",
    "Generator: The generator should be able to take a batch of videos as input without any error. Steps like cropping, resizing and normalization should be performed successfully.\n",
    "\n",
    "Model: Develop a model that is able to train without any errors which will be judged on the total number of parameters (as the inference(prediction) time should be less) and the accuracy achieved. As suggested by Snehansu, start training on a small amount of data and then proceed further.\n",
    "\n",
    "Write up: This should contain the detailed procedure followed in choosing the final model. The write up should start with the reason for choosing the base model, then highlight the reasons and metrics taken into consideration to modify and experiment to arrive at the final model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sun May 15 10:06:52 2022       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 470.42.01    Driver Version: 470.42.01    CUDA Version: 11.4     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                               |                      |               MIG M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  Quadro RTX 6000     Off  | 00000000:41:00.0 Off |                  Off |\n",
      "| 33%   28C    P8    11W / 260W |      5MiB / 24220MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                                  |\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
      "|        ID   ID                                                   Usage      |\n",
      "|=============================================================================|\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "## Checking the GPU configuration\n",
    "\n",
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Batch size 20, image 120X 120"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generator\n",
    "This is one of the most important part of the code. The overall structure of the generator has been given. In the generator, you are going to preprocess the images as you have images of 2 different dimensions as well as create a batch of video frames. You have to experiment with `img_idx`, `y`,`z` and normalization such that you get high accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_doc = np.random.permutation(open('datasets/Project_data/train.csv').readlines())\n",
    "val_doc = np.random.permutation(open('datasets/Project_data/val.csv').readlines())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 20 #experiment with the batch size\n",
    "x = 30\n",
    "y = 120\n",
    "z = 120\n",
    "\n",
    "def generator(source_path, folder_list, batch_size):\n",
    "    print( 'Source path = ', source_path, '; batch size =', batch_size)\n",
    "    img_idx = [x for x in range(0,x)] #create a list of image numbers you want to use for a particular video\n",
    "    while True:\n",
    "        t = np.random.permutation(folder_list)\n",
    "        num_batches = len(folder_list)//batch_size # calculate the number of batches\n",
    "        for batch in range(num_batches): # we iterate over the number of batches\n",
    "            batch_data = np.zeros((batch_size,x,y,z,3)) # x is the number of images you use for each video, (y,z) is the final size of the input images and 3 is the number of channels RGB\n",
    "            batch_labels = np.zeros((batch_size,5)) # batch_labels is the one hot representation of the output\n",
    "            for folder in range(batch_size): # iterate over the batch_size\n",
    "                imgs = os.listdir(source_path+'/'+ t[folder + (batch*batch_size)].split(';')[0]) # read all the images in the folder\n",
    "                for idx,item in enumerate(img_idx): #  Iterate iver the frames/images of a folder to read them in\n",
    "                    image = imread(source_path+'/'+ t[folder + (batch*batch_size)].strip().split(';')[0]+'/'+imgs[item]).astype(np.float32)\n",
    "                    \n",
    "                    #crop the images and resize them. Note that the images are of 2 different shape \n",
    "                    \n",
    "                    resized_image = imresize(image,(y,z))\n",
    "                    resized_image = resized_image/255\n",
    "                         \n",
    "                    #and the conv3D will throw error if the inputs in a batch have different shapes\n",
    "                    \n",
    "                    \n",
    "                    \n",
    "                    batch_data[folder,idx,:,:,0] = (resized_image[:,:,0]) #normalise and feed in the image\n",
    "                    batch_data[folder,idx,:,:,1] = (resized_image[:,:,1]) #normalise and feed in the image\n",
    "                    batch_data[folder,idx,:,:,2] = (resized_image[:,:,2]) #normalise and feed in the image\n",
    "                    \n",
    "                batch_labels[folder, int(t[folder + (batch*batch_size)].strip().split(';')[2])] = 1\n",
    "            yield batch_data, batch_labels #you yield the batch_data and the batch_labels, remember what does yield do\n",
    "\n",
    "        \n",
    "        # write the code for the remaining data points which are left after full batches\n",
    "        if(len(folder_list) != batch_size*num_batches):\n",
    "            batch_size = len(folder_list) - (batch_size*num_batches)\n",
    "            batch_data = np.zeros((batch_size,x,y,z,3))\n",
    "            batch_labels = np.zeros((batch_size,5))\n",
    "            for folder in range(batch_size):\n",
    "                imgs = os.listdir(source_path+'/'+ t[folder + (batch*batch_size)].split(';')[0]) # read all the images in the folder\n",
    "                for idx,item in enumerate(img_idx): #  Iterate iver the frames/images of a folder to read them in\n",
    "                    image = imread(source_path+'/'+ t[folder + (batch*batch_size)].strip().split(';')[0]+'/'+imgs[item]).astype(np.float32)\n",
    "                    \n",
    "                    #crop the images and resize them. Note that the images are of 2 different shape \n",
    "                    #and the conv3D will throw error if the inputs in a batch have different shapes\n",
    "                    resized_image = imresize(image,(y,z))\n",
    "                    resized_image = resized_image/255\n",
    "                    \n",
    "                    batch_data[folder,idx,:,:,0] = (resized_image[:,:,0])#normalise and feed in the image\n",
    "                    batch_data[folder,idx,:,:,1] = (resized_image[:,:,1])#normalise and feed in the image\n",
    "                    batch_data[folder,idx,:,:,2] = (resized_image[:,:,2])#normalise and feed in the image\n",
    "                    \n",
    "                batch_labels[folder, int(t[folder + (batch*batch_size)].strip().split(';')[2])] = 1\n",
    "            yield batch_data, batch_labels #you yield the batch_data and the batch_labels, remember what does yield do\n",
    "        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note here that a video is represented above in the generator as (number of images, height, width, number of channels). Take this into consideration while creating the model architecture."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# training sequences = 663\n",
      "# validation sequences = 100\n",
      "# epochs = 35\n"
     ]
    }
   ],
   "source": [
    "curr_dt_time = datetime.datetime.now()\n",
    "train_path = 'datasets/Project_data/train'\n",
    "val_path = 'datasets/Project_data/val'\n",
    "num_train_sequences = len(train_doc)\n",
    "print('# training sequences =', num_train_sequences)\n",
    "num_val_sequences = len(val_doc)\n",
    "print('# validation sequences =', num_val_sequences)\n",
    "num_epochs = 35 # choose the number of epochs\n",
    "print ('# epochs =', num_epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model 1: 3D Conv SGD Optimizer\n",
    "Here you make the model using different functionalities that Keras provides. Remember to use `Conv3D` and `MaxPooling3D` and not `Conv2D` and `Maxpooling2D` for a 3D convolution model. You would want to use `TimeDistributed` while building a Conv2D + RNN model. Also remember that the last layer is the softmax. Design the network in such a way that the model is able to give good accuracy on the least number of parameters so that it can fit in the memory of the webcam."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-05-15 10:06:55.948274: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:39] Overriding allow_growth setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.\n",
      "2022-05-15 10:06:55.948366: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1525] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 22846 MB memory:  -> device: 0, name: Quadro RTX 6000, pci bus id: 0000:41:00.0, compute capability: 7.5\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.models import Sequential, Model\n",
    "from tensorflow.keras.layers import Dense, GRU, Flatten, TimeDistributed, Flatten, BatchNormalization, Activation,Dropout\n",
    "from tensorflow.keras.layers import Conv3D, MaxPooling3D\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, ReduceLROnPlateau\n",
    "from tensorflow.keras import optimizers\n",
    "\n",
    "#write your model here\n",
    "#Updation in Generator as well\n",
    "x = 15  #Updation in Generator as well\n",
    "y = 64\n",
    "z = 64 \n",
    "\n",
    "#Input layer\n",
    "model = Sequential()\n",
    "model.add(Conv3D(64, (3,3,3), strides=(1,1,1), padding='same', input_shape=(x,y,z,3),activation='relu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(MaxPooling3D(pool_size=(2,2,2), strides=(2,2,2)))\n",
    "\n",
    "#Hidden Layer\n",
    "model.add(Conv3D(128, (3,3,3), strides=(1,1,1), padding='same',activation='relu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(MaxPooling3D(pool_size=(2,2,2), strides=(2,2,2)))\n",
    "\n",
    "# model.add(Dropout(0.25))\n",
    "\n",
    "model.add(Conv3D(256, (3,3,3), strides=(1,1,1), padding='same',activation='relu'))\n",
    "model.add(BatchNormalization())\n",
    "\n",
    "# model.add(Dropout(0.25))\n",
    "\n",
    "model.add(Conv3D(256, (3,3,3), strides=(1,1,1), padding='same',activation='relu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(MaxPooling3D(pool_size=(2,2,2), strides=(2,2,2)))\n",
    "\n",
    "#Dense and output layer.\n",
    "model.add(Flatten())\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(512, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(5, activation='softmax'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that you have written the model, the next step is to `compile` the model. When you print the `summary` of the model, you'll see the total number of parameters you have to train."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv3d (Conv3D)             (None, 15, 64, 64, 64)    5248      \n",
      "                                                                 \n",
      " batch_normalization (BatchN  (None, 15, 64, 64, 64)   256       \n",
      " ormalization)                                                   \n",
      "                                                                 \n",
      " max_pooling3d (MaxPooling3D  (None, 7, 32, 32, 64)    0         \n",
      " )                                                               \n",
      "                                                                 \n",
      " conv3d_1 (Conv3D)           (None, 7, 32, 32, 128)    221312    \n",
      "                                                                 \n",
      " batch_normalization_1 (Batc  (None, 7, 32, 32, 128)   512       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " max_pooling3d_1 (MaxPooling  (None, 3, 16, 16, 128)   0         \n",
      " 3D)                                                             \n",
      "                                                                 \n",
      " conv3d_2 (Conv3D)           (None, 3, 16, 16, 256)    884992    \n",
      "                                                                 \n",
      " batch_normalization_2 (Batc  (None, 3, 16, 16, 256)   1024      \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " conv3d_3 (Conv3D)           (None, 3, 16, 16, 256)    1769728   \n",
      "                                                                 \n",
      " batch_normalization_3 (Batc  (None, 3, 16, 16, 256)   1024      \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " max_pooling3d_2 (MaxPooling  (None, 1, 8, 8, 256)     0         \n",
      " 3D)                                                             \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 16384)             0         \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 16384)             0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 512)               8389120   \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (None, 512)               0         \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 5)                 2565      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 11,275,781\n",
      "Trainable params: 11,274,373\n",
      "Non-trainable params: 1,408\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "\n",
    "optimiser =  optimizers.SGD(learning_rate=0.01, decay=1e-5, momentum=0.7, nesterov=True) #write your optimizer\n",
    "model.compile(optimizer=optimiser, loss='categorical_crossentropy', metrics=['categorical_accuracy'])\n",
    "print (model.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us create the `train_generator` and the `val_generator` which will be used in `.fit_generator`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_generator = generator(train_path, train_doc, batch_size)\n",
    "val_generator = generator(val_path, val_doc, batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:`period` argument is deprecated. Please use `save_freq` to specify the frequency in number of batches seen.\n"
     ]
    }
   ],
   "source": [
    "model_name = 'model_init' + '_' + str(curr_dt_time).replace(' ','').replace(':','_') + '/'\n",
    "    \n",
    "if not os.path.exists(model_name):\n",
    "    os.mkdir(model_name)\n",
    "        \n",
    "filepath = model_name + 'model-{epoch:05d}-{loss:.5f}-{categorical_accuracy:.5f}-{val_loss:.5f}-{val_categorical_accuracy:.5f}.h5'\n",
    "\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='val_loss', verbose=1, save_best_only=True, save_weights_only=False, mode='auto', period=1)\n",
    "\n",
    "LR = ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=2, verbose=1, mode='min', min_delta=0.0001, cooldown=0, min_lr=0.00001) # write the REducelronplateau code here\n",
    "# LR=ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=2, cooldown=1, verbose=1)\n",
    "callbacks_list = [checkpoint, LR]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `steps_per_epoch` and `validation_steps` are used by `fit_generator` to decide the number of next() calls it need to make."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "if (num_train_sequences%batch_size) == 0:\n",
    "    steps_per_epoch = int(num_train_sequences/batch_size)\n",
    "else:\n",
    "    steps_per_epoch = (num_train_sequences//batch_size) + 1\n",
    "\n",
    "if (num_val_sequences%batch_size) == 0:\n",
    "    validation_steps = int(num_val_sequences/batch_size)\n",
    "else:\n",
    "    validation_steps = (num_val_sequences//batch_size) + 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us now fit the model. This will start training the model and with the help of the checkpoints, you'll be able to save the model at the end of each epoch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_375/385748685.py:1: UserWarning: `Model.fit_generator` is deprecated and will be removed in a future version. Please use `Model.fit`, which supports generators.\n",
      "  model.fit_generator(train_generator, steps_per_epoch=steps_per_epoch, epochs=num_epochs, verbose=1,\n",
      "/tmp/ipykernel_375/1229740096.py:18: DeprecationWarning:     `imread` is deprecated!\n",
      "    `imread` is deprecated in SciPy 1.0.0, and will be removed in 1.2.0.\n",
      "    Use ``imageio.imread`` instead.\n",
      "  image = imread(source_path+'/'+ t[folder + (batch*batch_size)].strip().split(';')[0]+'/'+imgs[item]).astype(np.float32)\n",
      "/tmp/ipykernel_375/1229740096.py:22: DeprecationWarning:     `imresize` is deprecated!\n",
      "    `imresize` is deprecated in SciPy 1.0.0, and will be removed in 1.2.0.\n",
      "    Use ``skimage.transform.resize`` instead.\n",
      "  resized_image = imresize(image,(y,z))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Source path =  datasets/Project_data/train ; batch size = 20\n",
      "Epoch 1/35\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-05-15 10:07:06.174415: I tensorflow/stream_executor/cuda/cuda_dnn.cc:377] Loaded cuDNN version 8302\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "33/34 [============================>.] - ETA: 0s - loss: 8.0406 - categorical_accuracy: 0.2682"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_375/1229740096.py:45: DeprecationWarning:     `imread` is deprecated!\n",
      "    `imread` is deprecated in SciPy 1.0.0, and will be removed in 1.2.0.\n",
      "    Use ``imageio.imread`` instead.\n",
      "  image = imread(source_path+'/'+ t[folder + (batch*batch_size)].strip().split(';')[0]+'/'+imgs[item]).astype(np.float32)\n",
      "/tmp/ipykernel_375/1229740096.py:49: DeprecationWarning:     `imresize` is deprecated!\n",
      "    `imresize` is deprecated in SciPy 1.0.0, and will be removed in 1.2.0.\n",
      "    Use ``skimage.transform.resize`` instead.\n",
      "  resized_image = imresize(image,(y,z))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "34/34 [==============================] - ETA: 0s - loss: 8.0155 - categorical_accuracy: 0.2670Source path =  datasets/Project_data/val ; batch size = 20\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 66.24117, saving model to model_init_2022-05-1510_06_54.685625/model-00001-8.01551-0.26697-66.24117-0.18000.h5\n",
      "34/34 [==============================] - 41s 1s/step - loss: 8.0155 - categorical_accuracy: 0.2670 - val_loss: 66.2412 - val_categorical_accuracy: 0.1800 - lr: 0.0100\n",
      "Epoch 2/35\n",
      "34/34 [==============================] - ETA: 0s - loss: 14.4410 - categorical_accuracy: 0.2843\n",
      "Epoch 00002: val_loss did not improve from 66.24117\n",
      "34/34 [==============================] - 12s 350ms/step - loss: 14.4410 - categorical_accuracy: 0.2843 - val_loss: 80.9734 - val_categorical_accuracy: 0.1600 - lr: 0.0100\n",
      "Epoch 3/35\n",
      "34/34 [==============================] - ETA: 0s - loss: 8.7770 - categorical_accuracy: 0.2549\n",
      "Epoch 00003: val_loss improved from 66.24117 to 17.91578, saving model to model_init_2022-05-1510_06_54.685625/model-00003-8.77702-0.25490-17.91578-0.21000.h5\n",
      "34/34 [==============================] - 13s 403ms/step - loss: 8.7770 - categorical_accuracy: 0.2549 - val_loss: 17.9158 - val_categorical_accuracy: 0.2100 - lr: 0.0100\n",
      "Epoch 4/35\n",
      "34/34 [==============================] - ETA: 0s - loss: 2.4263 - categorical_accuracy: 0.1863\n",
      "Epoch 00004: val_loss improved from 17.91578 to 4.68323, saving model to model_init_2022-05-1510_06_54.685625/model-00004-2.42625-0.18627-4.68323-0.25000.h5\n",
      "34/34 [==============================] - 13s 383ms/step - loss: 2.4263 - categorical_accuracy: 0.1863 - val_loss: 4.6832 - val_categorical_accuracy: 0.2500 - lr: 0.0100\n",
      "Epoch 5/35\n",
      "34/34 [==============================] - ETA: 0s - loss: 2.5569 - categorical_accuracy: 0.1569\n",
      "Epoch 00005: val_loss improved from 4.68323 to 1.90776, saving model to model_init_2022-05-1510_06_54.685625/model-00005-2.55686-0.15686-1.90776-0.25000.h5\n",
      "34/34 [==============================] - 12s 372ms/step - loss: 2.5569 - categorical_accuracy: 0.1569 - val_loss: 1.9078 - val_categorical_accuracy: 0.2500 - lr: 0.0100\n",
      "Epoch 6/35\n",
      "34/34 [==============================] - ETA: 0s - loss: 1.8006 - categorical_accuracy: 0.1765\n",
      "Epoch 00006: val_loss improved from 1.90776 to 1.88983, saving model to model_init_2022-05-1510_06_54.685625/model-00006-1.80063-0.17647-1.88983-0.22000.h5\n",
      "34/34 [==============================] - 12s 371ms/step - loss: 1.8006 - categorical_accuracy: 0.1765 - val_loss: 1.8898 - val_categorical_accuracy: 0.2200 - lr: 0.0100\n",
      "Epoch 7/35\n",
      "34/34 [==============================] - ETA: 0s - loss: 1.8666 - categorical_accuracy: 0.2255\n",
      "Epoch 00007: val_loss improved from 1.88983 to 1.72054, saving model to model_init_2022-05-1510_06_54.685625/model-00007-1.86662-0.22549-1.72054-0.26000.h5\n",
      "34/34 [==============================] - 13s 394ms/step - loss: 1.8666 - categorical_accuracy: 0.2255 - val_loss: 1.7205 - val_categorical_accuracy: 0.2600 - lr: 0.0100\n",
      "Epoch 8/35\n",
      "34/34 [==============================] - ETA: 0s - loss: 1.6167 - categorical_accuracy: 0.1667\n",
      "Epoch 00008: val_loss improved from 1.72054 to 1.63510, saving model to model_init_2022-05-1510_06_54.685625/model-00008-1.61674-0.16667-1.63510-0.23000.h5\n",
      "34/34 [==============================] - 12s 359ms/step - loss: 1.6167 - categorical_accuracy: 0.1667 - val_loss: 1.6351 - val_categorical_accuracy: 0.2300 - lr: 0.0100\n",
      "Epoch 9/35\n",
      "34/34 [==============================] - ETA: 0s - loss: 1.7230 - categorical_accuracy: 0.2157\n",
      "Epoch 00009: val_loss improved from 1.63510 to 1.62302, saving model to model_init_2022-05-1510_06_54.685625/model-00009-1.72300-0.21569-1.62302-0.16000.h5\n",
      "34/34 [==============================] - 12s 369ms/step - loss: 1.7230 - categorical_accuracy: 0.2157 - val_loss: 1.6230 - val_categorical_accuracy: 0.1600 - lr: 0.0100\n",
      "Epoch 10/35\n",
      "34/34 [==============================] - ETA: 0s - loss: 1.6195 - categorical_accuracy: 0.1863\n",
      "Epoch 00010: val_loss improved from 1.62302 to 1.61547, saving model to model_init_2022-05-1510_06_54.685625/model-00010-1.61950-0.18627-1.61547-0.22000.h5\n",
      "34/34 [==============================] - 13s 394ms/step - loss: 1.6195 - categorical_accuracy: 0.1863 - val_loss: 1.6155 - val_categorical_accuracy: 0.2200 - lr: 0.0100\n",
      "Epoch 11/35\n",
      "34/34 [==============================] - ETA: 0s - loss: 1.6115 - categorical_accuracy: 0.2549\n",
      "Epoch 00011: val_loss improved from 1.61547 to 1.60944, saving model to model_init_2022-05-1510_06_54.685625/model-00011-1.61153-0.25490-1.60944-0.22000.h5\n",
      "34/34 [==============================] - 12s 359ms/step - loss: 1.6115 - categorical_accuracy: 0.2549 - val_loss: 1.6094 - val_categorical_accuracy: 0.2200 - lr: 0.0100\n",
      "Epoch 12/35\n",
      "34/34 [==============================] - ETA: 0s - loss: 1.6201 - categorical_accuracy: 0.1667\n",
      "Epoch 00012: val_loss did not improve from 1.60944\n",
      "34/34 [==============================] - 13s 386ms/step - loss: 1.6201 - categorical_accuracy: 0.1667 - val_loss: 1.6114 - val_categorical_accuracy: 0.1800 - lr: 0.0100\n",
      "Epoch 13/35\n",
      "34/34 [==============================] - ETA: 0s - loss: 1.6287 - categorical_accuracy: 0.1275\n",
      "Epoch 00013: val_loss improved from 1.60944 to 1.59782, saving model to model_init_2022-05-1510_06_54.685625/model-00013-1.62865-0.12745-1.59782-0.23000.h5\n",
      "34/34 [==============================] - 12s 376ms/step - loss: 1.6287 - categorical_accuracy: 0.1275 - val_loss: 1.5978 - val_categorical_accuracy: 0.2300 - lr: 0.0100\n",
      "Epoch 14/35\n",
      "34/34 [==============================] - ETA: 0s - loss: 1.9567 - categorical_accuracy: 0.1961\n",
      "Epoch 00014: val_loss did not improve from 1.59782\n",
      "34/34 [==============================] - 12s 375ms/step - loss: 1.9567 - categorical_accuracy: 0.1961 - val_loss: 14.3594 - val_categorical_accuracy: 0.2800 - lr: 0.0100\n",
      "Epoch 15/35\n",
      "34/34 [==============================] - ETA: 0s - loss: 2.0317 - categorical_accuracy: 0.1275\n",
      "Epoch 00015: val_loss did not improve from 1.59782\n",
      "\n",
      "Epoch 00015: ReduceLROnPlateau reducing learning rate to 0.0009999999776482583.\n",
      "34/34 [==============================] - 12s 356ms/step - loss: 2.0317 - categorical_accuracy: 0.1275 - val_loss: 1.6520 - val_categorical_accuracy: 0.2100 - lr: 0.0100\n",
      "Epoch 16/35\n",
      "34/34 [==============================] - ETA: 0s - loss: 1.6103 - categorical_accuracy: 0.2059\n",
      "Epoch 00016: val_loss did not improve from 1.59782\n",
      "34/34 [==============================] - 13s 379ms/step - loss: 1.6103 - categorical_accuracy: 0.2059 - val_loss: 1.7111 - val_categorical_accuracy: 0.1900 - lr: 1.0000e-03\n",
      "Epoch 17/35\n",
      "34/34 [==============================] - ETA: 0s - loss: 1.6093 - categorical_accuracy: 0.1765\n",
      "Epoch 00017: val_loss did not improve from 1.59782\n",
      "\n",
      "Epoch 00017: ReduceLROnPlateau reducing learning rate to 9.999999310821295e-05.\n",
      "34/34 [==============================] - 12s 364ms/step - loss: 1.6093 - categorical_accuracy: 0.1765 - val_loss: 1.6855 - val_categorical_accuracy: 0.2400 - lr: 1.0000e-03\n",
      "Epoch 18/35\n",
      "34/34 [==============================] - ETA: 0s - loss: 1.6053 - categorical_accuracy: 0.2353\n",
      "Epoch 00018: val_loss did not improve from 1.59782\n",
      "34/34 [==============================] - 13s 384ms/step - loss: 1.6053 - categorical_accuracy: 0.2353 - val_loss: 1.7477 - val_categorical_accuracy: 0.1600 - lr: 1.0000e-04\n",
      "Epoch 19/35\n",
      "34/34 [==============================] - ETA: 0s - loss: 1.6195 - categorical_accuracy: 0.1961\n",
      "Epoch 00019: val_loss did not improve from 1.59782\n",
      "\n",
      "Epoch 00019: ReduceLROnPlateau reducing learning rate to 1e-05.\n",
      "34/34 [==============================] - 12s 371ms/step - loss: 1.6195 - categorical_accuracy: 0.1961 - val_loss: 1.6280 - val_categorical_accuracy: 0.1800 - lr: 1.0000e-04\n",
      "Epoch 20/35\n",
      "34/34 [==============================] - ETA: 0s - loss: 1.6119 - categorical_accuracy: 0.1765\n",
      "Epoch 00020: val_loss did not improve from 1.59782\n",
      "34/34 [==============================] - 12s 359ms/step - loss: 1.6119 - categorical_accuracy: 0.1765 - val_loss: 1.7421 - val_categorical_accuracy: 0.1700 - lr: 1.0000e-05\n",
      "Epoch 21/35\n",
      "34/34 [==============================] - ETA: 0s - loss: 1.6927 - categorical_accuracy: 0.2549\n",
      "Epoch 00021: val_loss did not improve from 1.59782\n",
      "34/34 [==============================] - 11s 344ms/step - loss: 1.6927 - categorical_accuracy: 0.2549 - val_loss: 1.7661 - val_categorical_accuracy: 0.1700 - lr: 1.0000e-05\n",
      "Epoch 22/35\n",
      "34/34 [==============================] - ETA: 0s - loss: 1.7753 - categorical_accuracy: 0.2059\n",
      "Epoch 00022: val_loss did not improve from 1.59782\n",
      "34/34 [==============================] - 13s 385ms/step - loss: 1.7753 - categorical_accuracy: 0.2059 - val_loss: 1.7684 - val_categorical_accuracy: 0.1800 - lr: 1.0000e-05\n",
      "Epoch 23/35\n",
      "34/34 [==============================] - ETA: 0s - loss: 1.6284 - categorical_accuracy: 0.2255\n",
      "Epoch 00023: val_loss did not improve from 1.59782\n",
      "34/34 [==============================] - 12s 375ms/step - loss: 1.6284 - categorical_accuracy: 0.2255 - val_loss: 1.7701 - val_categorical_accuracy: 0.1600 - lr: 1.0000e-05\n",
      "Epoch 24/35\n",
      "34/34 [==============================] - ETA: 0s - loss: 2.2830 - categorical_accuracy: 0.1961\n",
      "Epoch 00024: val_loss did not improve from 1.59782\n",
      "34/34 [==============================] - 12s 369ms/step - loss: 2.2830 - categorical_accuracy: 0.1961 - val_loss: 1.7466 - val_categorical_accuracy: 0.2200 - lr: 1.0000e-05\n",
      "Epoch 25/35\n",
      "34/34 [==============================] - ETA: 0s - loss: 1.6079 - categorical_accuracy: 0.2255\n",
      "Epoch 00025: val_loss did not improve from 1.59782\n",
      "34/34 [==============================] - 13s 386ms/step - loss: 1.6079 - categorical_accuracy: 0.2255 - val_loss: 1.7661 - val_categorical_accuracy: 0.1900 - lr: 1.0000e-05\n",
      "Epoch 26/35\n",
      "34/34 [==============================] - ETA: 0s - loss: 1.6017 - categorical_accuracy: 0.2353\n",
      "Epoch 00026: val_loss did not improve from 1.59782\n",
      "34/34 [==============================] - 12s 349ms/step - loss: 1.6017 - categorical_accuracy: 0.2353 - val_loss: 1.7658 - val_categorical_accuracy: 0.1800 - lr: 1.0000e-05\n",
      "Epoch 27/35\n",
      "34/34 [==============================] - ETA: 0s - loss: 1.6190 - categorical_accuracy: 0.1471\n",
      "Epoch 00027: val_loss did not improve from 1.59782\n",
      "34/34 [==============================] - 13s 390ms/step - loss: 1.6190 - categorical_accuracy: 0.1471 - val_loss: 1.7510 - val_categorical_accuracy: 0.2000 - lr: 1.0000e-05\n",
      "Epoch 28/35\n",
      "34/34 [==============================] - ETA: 0s - loss: 1.6038 - categorical_accuracy: 0.2353\n",
      "Epoch 00028: val_loss did not improve from 1.59782\n",
      "34/34 [==============================] - 12s 354ms/step - loss: 1.6038 - categorical_accuracy: 0.2353 - val_loss: 1.7608 - val_categorical_accuracy: 0.1500 - lr: 1.0000e-05\n",
      "Epoch 29/35\n",
      "34/34 [==============================] - ETA: 0s - loss: 1.6106 - categorical_accuracy: 0.1569\n",
      "Epoch 00029: val_loss did not improve from 1.59782\n",
      "34/34 [==============================] - 13s 382ms/step - loss: 1.6106 - categorical_accuracy: 0.1569 - val_loss: 1.6063 - val_categorical_accuracy: 0.1400 - lr: 1.0000e-05\n",
      "Epoch 30/35\n",
      "34/34 [==============================] - ETA: 0s - loss: 1.6084 - categorical_accuracy: 0.2451\n",
      "Epoch 00030: val_loss did not improve from 1.59782\n",
      "34/34 [==============================] - 11s 346ms/step - loss: 1.6084 - categorical_accuracy: 0.2451 - val_loss: 1.9341 - val_categorical_accuracy: 0.2000 - lr: 1.0000e-05\n",
      "Epoch 31/35\n",
      "34/34 [==============================] - ETA: 0s - loss: 1.6147 - categorical_accuracy: 0.1863\n",
      "Epoch 00031: val_loss did not improve from 1.59782\n",
      "34/34 [==============================] - 12s 367ms/step - loss: 1.6147 - categorical_accuracy: 0.1863 - val_loss: 1.7690 - val_categorical_accuracy: 0.1800 - lr: 1.0000e-05\n",
      "Epoch 32/35\n",
      "34/34 [==============================] - ETA: 0s - loss: 1.6074 - categorical_accuracy: 0.2843\n",
      "Epoch 00032: val_loss improved from 1.59782 to 1.59104, saving model to model_init_2022-05-1510_06_54.685625/model-00032-1.60740-0.28431-1.59104-0.19000.h5\n",
      "34/34 [==============================] - 12s 375ms/step - loss: 1.6074 - categorical_accuracy: 0.2843 - val_loss: 1.5910 - val_categorical_accuracy: 0.1900 - lr: 1.0000e-05\n",
      "Epoch 33/35\n",
      "34/34 [==============================] - ETA: 0s - loss: 1.6754 - categorical_accuracy: 0.1275\n",
      "Epoch 00033: val_loss did not improve from 1.59104\n",
      "34/34 [==============================] - 12s 374ms/step - loss: 1.6754 - categorical_accuracy: 0.1275 - val_loss: 1.9586 - val_categorical_accuracy: 0.1600 - lr: 1.0000e-05\n",
      "Epoch 34/35\n",
      "34/34 [==============================] - ETA: 0s - loss: 1.5979 - categorical_accuracy: 0.2157\n",
      "Epoch 00034: val_loss did not improve from 1.59104\n",
      "34/34 [==============================] - 12s 376ms/step - loss: 1.5979 - categorical_accuracy: 0.2157 - val_loss: 1.7660 - val_categorical_accuracy: 0.2000 - lr: 1.0000e-05\n",
      "Epoch 35/35\n",
      "34/34 [==============================] - ETA: 0s - loss: 2.0929 - categorical_accuracy: 0.2157\n",
      "Epoch 00035: val_loss did not improve from 1.59104\n",
      "34/34 [==============================] - 12s 354ms/step - loss: 2.0929 - categorical_accuracy: 0.2157 - val_loss: 1.5930 - val_categorical_accuracy: 0.1500 - lr: 1.0000e-05\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f8442ce9f40>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit_generator(train_generator, steps_per_epoch=steps_per_epoch, epochs=num_epochs, verbose=1, \n",
    "                    callbacks=callbacks_list, validation_data=val_generator, \n",
    "                    validation_steps=validation_steps, class_weight=None, workers=1, initial_epoch=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model 2: 3D Conv - Adam optimizer epoch 25 batch size 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv3d_4 (Conv3D)           (None, 30, 120, 120, 8)   656       \n",
      "                                                                 \n",
      " batch_normalization_4 (Batc  (None, 30, 120, 120, 8)  32        \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " activation (Activation)     (None, 30, 120, 120, 8)   0         \n",
      "                                                                 \n",
      " conv3d_5 (Conv3D)           (None, 30, 120, 120, 16)  3472      \n",
      "                                                                 \n",
      " activation_1 (Activation)   (None, 30, 120, 120, 16)  0         \n",
      "                                                                 \n",
      " batch_normalization_5 (Batc  (None, 30, 120, 120, 16)  64       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " max_pooling3d_3 (MaxPooling  (None, 15, 60, 60, 16)   0         \n",
      " 3D)                                                             \n",
      "                                                                 \n",
      " conv3d_6 (Conv3D)           (None, 15, 60, 60, 32)    4128      \n",
      "                                                                 \n",
      " activation_2 (Activation)   (None, 15, 60, 60, 32)    0         \n",
      "                                                                 \n",
      " batch_normalization_6 (Batc  (None, 15, 60, 60, 32)   128       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " max_pooling3d_4 (MaxPooling  (None, 7, 30, 30, 32)    0         \n",
      " 3D)                                                             \n",
      "                                                                 \n",
      " conv3d_7 (Conv3D)           (None, 7, 30, 30, 64)     16448     \n",
      "                                                                 \n",
      " activation_3 (Activation)   (None, 7, 30, 30, 64)     0         \n",
      "                                                                 \n",
      " batch_normalization_7 (Batc  (None, 7, 30, 30, 64)    256       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " max_pooling3d_5 (MaxPooling  (None, 3, 15, 15, 64)    0         \n",
      " 3D)                                                             \n",
      "                                                                 \n",
      " conv3d_8 (Conv3D)           (None, 3, 15, 15, 128)    65664     \n",
      "                                                                 \n",
      " activation_4 (Activation)   (None, 3, 15, 15, 128)    0         \n",
      "                                                                 \n",
      " batch_normalization_8 (Batc  (None, 3, 15, 15, 128)   512       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " max_pooling3d_6 (MaxPooling  (None, 1, 7, 7, 128)     0         \n",
      " 3D)                                                             \n",
      "                                                                 \n",
      " flatten_1 (Flatten)         (None, 6272)              0         \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 1000)              6273000   \n",
      "                                                                 \n",
      " dropout_2 (Dropout)         (None, 1000)              0         \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 500)               500500    \n",
      "                                                                 \n",
      " dropout_3 (Dropout)         (None, 500)               0         \n",
      "                                                                 \n",
      " dense_4 (Dense)             (None, 5)                 2505      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 6,867,365\n",
      "Trainable params: 6,866,869\n",
      "Non-trainable params: 496\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "# Let us experiment different x,y,z value in the CNN network and find tune all the image size & Hyperparameters later\n",
    "\n",
    "x = 30 # number of frames\n",
    "y = 120 # image width\n",
    "z = 120 # image height\n",
    "\n",
    "# Input all the images sequencial by building the layer with dropouts and batchnormalisation\n",
    "\n",
    "model_Three = Sequential()       \n",
    "model_Three.add(Conv3D(8,kernel_size=(3,3,3),input_shape=(x,y,z,3),padding='same'))\n",
    "model_Three.add(BatchNormalization())\n",
    "model_Three.add(Activation('relu'))\n",
    "\n",
    "model_Three.add(Conv3D(16, (3, 3, 3), padding='same'))\n",
    "model_Three.add(Activation('relu'))\n",
    "model_Three.add(BatchNormalization())\n",
    "model_Three.add(MaxPooling3D(pool_size=(2, 2, 2)))\n",
    "\n",
    "model_Three.add(Conv3D(32, (2, 2, 2), padding='same'))\n",
    "model_Three.add(Activation('relu'))\n",
    "model_Three.add(BatchNormalization())\n",
    "model_Three.add(MaxPooling3D(pool_size=(2, 2, 2)))\n",
    "\n",
    "model_Three.add(Conv3D(64, (2, 2, 2), padding='same'))\n",
    "model_Three.add(Activation('relu'))\n",
    "model_Three.add(BatchNormalization())\n",
    "model_Three.add(MaxPooling3D(pool_size=(2, 2, 2)))\n",
    "\n",
    "model_Three.add(Conv3D(128, (2, 2, 2), padding='same'))\n",
    "model_Three.add(Activation('relu'))\n",
    "model_Three.add(BatchNormalization())\n",
    "model_Three.add(MaxPooling3D(pool_size=(2, 2, 2)))      \n",
    "\n",
    "# Flatten layer \n",
    "\n",
    "model_Three.add(Flatten())\n",
    "\n",
    "model_Three.add(Dense(1000, activation='relu'))\n",
    "model_Three.add(Dropout(0.5))\n",
    "\n",
    "model_Three.add(Dense(500, activation='relu'))\n",
    "model_Three.add(Dropout(0.55))\n",
    "\n",
    "#Softmax layer\n",
    "\n",
    "model_Three.add(Dense(5, activation='softmax'))\n",
    "\n",
    "# Let us use the Adam optimiser \n",
    "\n",
    "optimiser = optimizers.Adam(lr=0.001) #write your optimizer\n",
    "model_Three.compile(optimizer=optimiser, loss='categorical_crossentropy', metrics=['categorical_accuracy'])\n",
    "print (model_Three.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let us train and validate the model \n",
    "train_generator = generator(train_path, train_doc, batch_size)\n",
    "val_generator = generator(val_path, val_doc, batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_375/3232244974.py:2: UserWarning: `Model.fit_generator` is deprecated and will be removed in a future version. Please use `Model.fit`, which supports generators.\n",
      "  model_Three.fit_generator(train_generator, steps_per_epoch=steps_per_epoch, epochs=25, verbose=1,\n",
      "/tmp/ipykernel_375/1229740096.py:18: DeprecationWarning:     `imread` is deprecated!\n",
      "    `imread` is deprecated in SciPy 1.0.0, and will be removed in 1.2.0.\n",
      "    Use ``imageio.imread`` instead.\n",
      "  image = imread(source_path+'/'+ t[folder + (batch*batch_size)].strip().split(';')[0]+'/'+imgs[item]).astype(np.float32)\n",
      "/tmp/ipykernel_375/1229740096.py:22: DeprecationWarning:     `imresize` is deprecated!\n",
      "    `imresize` is deprecated in SciPy 1.0.0, and will be removed in 1.2.0.\n",
      "    Use ``skimage.transform.resize`` instead.\n",
      "  resized_image = imresize(image,(y,z))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Source path =  datasets/Project_data/train ; batch size = 20\n",
      "Epoch 1/25\n",
      "32/34 [===========================>..] - ETA: 4s - loss: 6.8637 - categorical_accuracy: 0.3438"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_375/1229740096.py:45: DeprecationWarning:     `imread` is deprecated!\n",
      "    `imread` is deprecated in SciPy 1.0.0, and will be removed in 1.2.0.\n",
      "    Use ``imageio.imread`` instead.\n",
      "  image = imread(source_path+'/'+ t[folder + (batch*batch_size)].strip().split(';')[0]+'/'+imgs[item]).astype(np.float32)\n",
      "/tmp/ipykernel_375/1229740096.py:49: DeprecationWarning:     `imresize` is deprecated!\n",
      "    `imresize` is deprecated in SciPy 1.0.0, and will be removed in 1.2.0.\n",
      "    Use ``skimage.transform.resize`` instead.\n",
      "  resized_image = imresize(image,(y,z))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "34/34 [==============================] - ETA: 0s - loss: 6.7164 - categorical_accuracy: 0.3484Source path =  datasets/Project_data/val ; batch size = 20\n",
      "\n",
      "Epoch 00001: val_loss did not improve from 1.59104\n",
      "34/34 [==============================] - 89s 3s/step - loss: 6.7164 - categorical_accuracy: 0.3484 - val_loss: 6.2837 - val_categorical_accuracy: 0.2100 - lr: 0.0010\n",
      "Epoch 2/25\n",
      "34/34 [==============================] - ETA: 0s - loss: 4.1448 - categorical_accuracy: 0.3824\n",
      "Epoch 00002: val_loss did not improve from 1.59104\n",
      "34/34 [==============================] - 25s 763ms/step - loss: 4.1448 - categorical_accuracy: 0.3824 - val_loss: 7.8234 - val_categorical_accuracy: 0.2200 - lr: 0.0010\n",
      "Epoch 3/25\n",
      "34/34 [==============================] - ETA: 0s - loss: 4.6432 - categorical_accuracy: 0.2745\n",
      "Epoch 00003: val_loss did not improve from 1.59104\n",
      "\n",
      "Epoch 00003: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n",
      "34/34 [==============================] - 26s 783ms/step - loss: 4.6432 - categorical_accuracy: 0.2745 - val_loss: 10.3483 - val_categorical_accuracy: 0.2100 - lr: 0.0010\n",
      "Epoch 4/25\n",
      "34/34 [==============================] - ETA: 0s - loss: 3.3770 - categorical_accuracy: 0.2549\n",
      "Epoch 00004: val_loss did not improve from 1.59104\n",
      "34/34 [==============================] - 27s 825ms/step - loss: 3.3770 - categorical_accuracy: 0.2549 - val_loss: 10.8017 - val_categorical_accuracy: 0.1900 - lr: 1.0000e-04\n",
      "Epoch 5/25\n",
      "34/34 [==============================] - ETA: 0s - loss: 2.4372 - categorical_accuracy: 0.4118\n",
      "Epoch 00005: val_loss did not improve from 1.59104\n",
      "\n",
      "Epoch 00005: ReduceLROnPlateau reducing learning rate to 1.0000000474974514e-05.\n",
      "34/34 [==============================] - 27s 810ms/step - loss: 2.4372 - categorical_accuracy: 0.4118 - val_loss: 9.6949 - val_categorical_accuracy: 0.2100 - lr: 1.0000e-04\n",
      "Epoch 6/25\n",
      "34/34 [==============================] - ETA: 0s - loss: 1.7920 - categorical_accuracy: 0.4118\n",
      "Epoch 00006: val_loss did not improve from 1.59104\n",
      "34/34 [==============================] - 27s 829ms/step - loss: 1.7920 - categorical_accuracy: 0.4118 - val_loss: 8.6088 - val_categorical_accuracy: 0.2100 - lr: 1.0000e-05\n",
      "Epoch 7/25\n",
      "34/34 [==============================] - ETA: 0s - loss: 1.8036 - categorical_accuracy: 0.5000\n",
      "Epoch 00007: val_loss did not improve from 1.59104\n",
      "\n",
      "Epoch 00007: ReduceLROnPlateau reducing learning rate to 1e-05.\n",
      "34/34 [==============================] - 27s 815ms/step - loss: 1.8036 - categorical_accuracy: 0.5000 - val_loss: 7.4062 - val_categorical_accuracy: 0.2300 - lr: 1.0000e-05\n",
      "Epoch 8/25\n",
      "34/34 [==============================] - ETA: 0s - loss: 2.0104 - categorical_accuracy: 0.4118\n",
      "Epoch 00008: val_loss did not improve from 1.59104\n",
      "34/34 [==============================] - 27s 808ms/step - loss: 2.0104 - categorical_accuracy: 0.4118 - val_loss: 6.8103 - val_categorical_accuracy: 0.1600 - lr: 1.0000e-05\n",
      "Epoch 9/25\n",
      "34/34 [==============================] - ETA: 0s - loss: 1.8643 - categorical_accuracy: 0.4020\n",
      "Epoch 00009: val_loss did not improve from 1.59104\n",
      "34/34 [==============================] - 26s 799ms/step - loss: 1.8643 - categorical_accuracy: 0.4020 - val_loss: 4.7150 - val_categorical_accuracy: 0.2700 - lr: 1.0000e-05\n",
      "Epoch 10/25\n",
      "34/34 [==============================] - ETA: 0s - loss: 1.4242 - categorical_accuracy: 0.5490\n",
      "Epoch 00010: val_loss did not improve from 1.59104\n",
      "34/34 [==============================] - 28s 845ms/step - loss: 1.4242 - categorical_accuracy: 0.5490 - val_loss: 4.4012 - val_categorical_accuracy: 0.1900 - lr: 1.0000e-05\n",
      "Epoch 11/25\n",
      "34/34 [==============================] - ETA: 0s - loss: 1.7277 - categorical_accuracy: 0.4510\n",
      "Epoch 00011: val_loss did not improve from 1.59104\n",
      "34/34 [==============================] - 27s 807ms/step - loss: 1.7277 - categorical_accuracy: 0.4510 - val_loss: 3.2817 - val_categorical_accuracy: 0.2200 - lr: 1.0000e-05\n",
      "Epoch 12/25\n",
      "34/34 [==============================] - ETA: 0s - loss: 1.4850 - categorical_accuracy: 0.5196\n",
      "Epoch 00012: val_loss did not improve from 1.59104\n",
      "34/34 [==============================] - 27s 805ms/step - loss: 1.4850 - categorical_accuracy: 0.5196 - val_loss: 2.6575 - val_categorical_accuracy: 0.2800 - lr: 1.0000e-05\n",
      "Epoch 13/25\n",
      "34/34 [==============================] - ETA: 0s - loss: 1.6163 - categorical_accuracy: 0.4608\n",
      "Epoch 00013: val_loss did not improve from 1.59104\n",
      "34/34 [==============================] - 26s 792ms/step - loss: 1.6163 - categorical_accuracy: 0.4608 - val_loss: 1.6307 - val_categorical_accuracy: 0.4800 - lr: 1.0000e-05\n",
      "Epoch 14/25\n",
      "34/34 [==============================] - ETA: 0s - loss: 1.5498 - categorical_accuracy: 0.5000\n",
      "Epoch 00014: val_loss did not improve from 1.59104\n",
      "34/34 [==============================] - 27s 815ms/step - loss: 1.5498 - categorical_accuracy: 0.5000 - val_loss: 1.6368 - val_categorical_accuracy: 0.4500 - lr: 1.0000e-05\n",
      "Epoch 15/25\n",
      "34/34 [==============================] - ETA: 0s - loss: 1.6255 - categorical_accuracy: 0.5098\n",
      "Epoch 00015: val_loss improved from 1.59104 to 1.36407, saving model to model_init_2022-05-1510_06_54.685625/model-00015-1.62550-0.50980-1.36407-0.52000.h5\n",
      "34/34 [==============================] - 25s 766ms/step - loss: 1.6255 - categorical_accuracy: 0.5098 - val_loss: 1.3641 - val_categorical_accuracy: 0.5200 - lr: 1.0000e-05\n",
      "Epoch 16/25\n",
      "34/34 [==============================] - ETA: 0s - loss: 1.8285 - categorical_accuracy: 0.4020\n",
      "Epoch 00016: val_loss improved from 1.36407 to 1.22017, saving model to model_init_2022-05-1510_06_54.685625/model-00016-1.82846-0.40196-1.22017-0.55000.h5\n",
      "34/34 [==============================] - 27s 805ms/step - loss: 1.8285 - categorical_accuracy: 0.4020 - val_loss: 1.2202 - val_categorical_accuracy: 0.5500 - lr: 1.0000e-05\n",
      "Epoch 17/25\n",
      "34/34 [==============================] - ETA: 0s - loss: 1.6480 - categorical_accuracy: 0.3725\n",
      "Epoch 00017: val_loss improved from 1.22017 to 1.05465, saving model to model_init_2022-05-1510_06_54.685625/model-00017-1.64796-0.37255-1.05465-0.62000.h5\n",
      "34/34 [==============================] - 25s 766ms/step - loss: 1.6480 - categorical_accuracy: 0.3725 - val_loss: 1.0547 - val_categorical_accuracy: 0.6200 - lr: 1.0000e-05\n",
      "Epoch 18/25\n",
      "34/34 [==============================] - ETA: 0s - loss: 1.6584 - categorical_accuracy: 0.4902\n",
      "Epoch 00018: val_loss improved from 1.05465 to 1.04145, saving model to model_init_2022-05-1510_06_54.685625/model-00018-1.65843-0.49020-1.04145-0.58000.h5\n",
      "34/34 [==============================] - 26s 783ms/step - loss: 1.6584 - categorical_accuracy: 0.4902 - val_loss: 1.0415 - val_categorical_accuracy: 0.5800 - lr: 1.0000e-05\n",
      "Epoch 19/25\n",
      "34/34 [==============================] - ETA: 0s - loss: 1.2757 - categorical_accuracy: 0.5392\n",
      "Epoch 00019: val_loss improved from 1.04145 to 1.01620, saving model to model_init_2022-05-1510_06_54.685625/model-00019-1.27570-0.53922-1.01620-0.62000.h5\n",
      "34/34 [==============================] - 26s 774ms/step - loss: 1.2757 - categorical_accuracy: 0.5392 - val_loss: 1.0162 - val_categorical_accuracy: 0.6200 - lr: 1.0000e-05\n",
      "Epoch 20/25\n",
      "34/34 [==============================] - ETA: 0s - loss: 1.4118 - categorical_accuracy: 0.5588\n",
      "Epoch 00020: val_loss did not improve from 1.01620\n",
      "34/34 [==============================] - 27s 807ms/step - loss: 1.4118 - categorical_accuracy: 0.5588 - val_loss: 1.1029 - val_categorical_accuracy: 0.6300 - lr: 1.0000e-05\n",
      "Epoch 21/25\n",
      "34/34 [==============================] - ETA: 0s - loss: 1.5265 - categorical_accuracy: 0.5392\n",
      "Epoch 00021: val_loss improved from 1.01620 to 1.00371, saving model to model_init_2022-05-1510_06_54.685625/model-00021-1.52648-0.53922-1.00371-0.63000.h5\n",
      "34/34 [==============================] - 25s 763ms/step - loss: 1.5265 - categorical_accuracy: 0.5392 - val_loss: 1.0037 - val_categorical_accuracy: 0.6300 - lr: 1.0000e-05\n",
      "Epoch 22/25\n",
      "34/34 [==============================] - ETA: 0s - loss: 1.3710 - categorical_accuracy: 0.5000\n",
      "Epoch 00022: val_loss improved from 1.00371 to 0.95200, saving model to model_init_2022-05-1510_06_54.685625/model-00022-1.37103-0.50000-0.95200-0.62000.h5\n",
      "34/34 [==============================] - 25s 745ms/step - loss: 1.3710 - categorical_accuracy: 0.5000 - val_loss: 0.9520 - val_categorical_accuracy: 0.6200 - lr: 1.0000e-05\n",
      "Epoch 23/25\n",
      "34/34 [==============================] - ETA: 0s - loss: 1.6849 - categorical_accuracy: 0.4706\n",
      "Epoch 00023: val_loss improved from 0.95200 to 0.92877, saving model to model_init_2022-05-1510_06_54.685625/model-00023-1.68489-0.47059-0.92877-0.67000.h5\n",
      "34/34 [==============================] - 25s 759ms/step - loss: 1.6849 - categorical_accuracy: 0.4706 - val_loss: 0.9288 - val_categorical_accuracy: 0.6700 - lr: 1.0000e-05\n",
      "Epoch 24/25\n",
      "34/34 [==============================] - ETA: 0s - loss: 1.2904 - categorical_accuracy: 0.5588"
     ]
    }
   ],
   "source": [
    "## Let us fit the model\n",
    "model_Three.fit_generator(train_generator, steps_per_epoch=steps_per_epoch, epochs=25, verbose=1, \n",
    "                    callbacks=callbacks_list, validation_data=val_generator, \n",
    "                    validation_steps=validation_steps, class_weight=None, workers=1, initial_epoch=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model 3 VGGNET + RNN sgd optimizer epoch 35 batch size 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(30)\n",
    "rn.seed(30)\n",
    "tf.random.set_seed(30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Batch size\n",
    "batch_size = 40"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Defining the generator\n",
    "def generator_5(source_path, folder_list, batch_size):\n",
    "    print( 'Source path = ', source_path, '; batch size =', batch_size)\n",
    "    img_idx =[0,1,2,4,6,8,10,12,14,16,18,20,22,24,26,28]  #create a list of image numbers you want to use for a particular video\n",
    "    x = len(img_idx) #Number of images in a sequence.\n",
    "    y = 120 #Height of image\n",
    "    z = 120 #Width of image\n",
    "    while True:\n",
    "        t = np.random.permutation(folder_list)\n",
    "        num_batches = int(len(t)/batch_size) # calculate the number of batches\n",
    "        for batch in range(num_batches): # we iterate over the number of batches\n",
    "            batch_data = np.zeros((batch_size,x,y,z,3)) # x is the number of images you use for each video, (y,z) is the final size of the input images and 3 is the number of channels RGB\n",
    "            batch_labels = np.zeros((batch_size,5)) # batch_labels is the one hot representation of the output\n",
    "            for folder in range(batch_size): # iterate over the batch_size\n",
    "                imgs = os.listdir(source_path+'/'+ t[folder + (batch*batch_size)].split(';')[0]) # read all the images in the folder\n",
    "                for idx,item in enumerate(img_idx): #  Iterate iver the frames/images of a folder to read them in\n",
    "                    image = imread(source_path+'/'+ t[folder + (batch*batch_size)].strip().split(';')[0]+'/'+imgs[item])\n",
    "                    \n",
    "                    #crop the images and resize them. Note that the images are of 2 different shape \n",
    "                    #and the conv3D will throw error if the inputs in a batch have different shapes\n",
    "                    if image.shape[1] == 160:\n",
    "                        #Adjusting the width to 120 \n",
    "                        image = imresize(image[:,20:140,:], (y,z)).astype(np.float32)\n",
    "                    else:\n",
    "                        image = imresize(image, (y,z)).astype(np.float32)\n",
    "                    #Normalising using mean value of image.\n",
    "                    #Mean Red -122 ,Mean Green -113,Mean Blue -108\n",
    "                    batch_data[folder,idx,:,:,0] = image[:,:,0] - 122 #normalise and feed in the image\n",
    "                    batch_data[folder,idx,:,:,1] = image[:,:,1] - 113 #normalise and feed in the image\n",
    "                    batch_data[folder,idx,:,:,2] = image[:,:,2] - 108 #normalise and feed in the image                  \n",
    "                    \n",
    "                batch_labels[folder, int(t[folder + (batch*batch_size)].strip().split(';')[2])] = 1\n",
    "            yield batch_data, batch_labels #you yield the batch_data and the batch_labels, remember what does yield do\n",
    "\n",
    "        \n",
    "        # write the code for the remaining data points which are left after full batches\n",
    "        if (len(t)%batch_size) != 0:\n",
    "            batch_data = np.zeros((len(t)%batch_size,x,y,z,3))\n",
    "            batch_labels = np.zeros((len(t)%batch_size,5))\n",
    "            for folder in range(len(t)%batch_size):\n",
    "                imgs = os.listdir(source_path+'/'+ t[folder + (num_batches*batch_size)].split(';')[0])\n",
    "                for idx,item in enumerate(img_idx):\n",
    "                    image = imread(source_path+'/'+ t[folder + (num_batches*batch_size)].strip().split(';')[0]+'/'+imgs[item])\n",
    "                    \n",
    "                    if image.shape[1] == 160:\n",
    "                        #Adjusting the width to 120 \n",
    "                        image = imresize(image[:,20:140,:], (y,z)).astype(np.float32)\n",
    "                    else:\n",
    "                        image = imresize(image, (y,z)).astype(np.float32)\n",
    "                        \n",
    "                    batch_data[folder,idx,:,:,0] = image[:,:,0] - 122  #normalise and feed in the image\n",
    "                    batch_data[folder,idx,:,:,1] = image[:,:,1] - 113  #normalise and feed in the image\n",
    "                    batch_data[folder,idx,:,:,2] = image[:,:,2] - 108  #normalise and feed in the image\n",
    "                    \n",
    "                batch_labels[folder, int(t[folder + (num_batches*batch_size)].strip().split(';')[2])] = 1\n",
    "\n",
    "            yield batch_data, batch_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# epochs = 35\n"
     ]
    }
   ],
   "source": [
    "curr_dt_time = datetime.datetime.now()\n",
    "# choose the number of epochs\n",
    "num_epochs =35 \n",
    "print ('# epochs =', num_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.applications.vgg19 import VGG19\n",
    "#Model Building Code.\n",
    "img_seq = 16\n",
    "y=120\n",
    "z=120\n",
    "\n",
    "base_model = VGG19(include_top=False, weights='imagenet', input_shape=(y,z,3))\n",
    "x = base_model.output\n",
    "x = Flatten()(x)\n",
    "\n",
    "#x.add(Dropout(0.5))\n",
    "\n",
    "features = Dense(64, activation='relu')(x)\n",
    "conv_model = Model(inputs=base_model.input, outputs=features)\n",
    "    \n",
    "for layer in base_model.layers:\n",
    "    layer.trainable = False\n",
    "        \n",
    "model5 = Sequential()\n",
    "model5.add(TimeDistributed(conv_model, input_shape=(img_seq,y,z,3)))\n",
    "\n",
    "model5.add(GRU(32, return_sequences=True))\n",
    "model5.add(GRU(16))\n",
    "\n",
    "model5.add(Dropout(0.5))\n",
    "\n",
    "model5.add(Dense(64, activation='relu'))\n",
    "model5.add(Dense(5, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " time_distributed (TimeDistr  (None, 16, 64)           20319360  \n",
      " ibuted)                                                         \n",
      "                                                                 \n",
      " gru (GRU)                   (None, 16, 32)            9408      \n",
      "                                                                 \n",
      " gru_1 (GRU)                 (None, 16)                2400      \n",
      "                                                                 \n",
      " dropout_4 (Dropout)         (None, 16)                0         \n",
      "                                                                 \n",
      " dense_6 (Dense)             (None, 64)                1088      \n",
      "                                                                 \n",
      " dense_7 (Dense)             (None, 5)                 325       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 20,332,581\n",
      "Trainable params: 308,197\n",
      "Non-trainable params: 20,024,384\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "optimiser5 =  optimizers.SGD(learning_rate=0.01, decay=1e-5, momentum=0.7, nesterov=True) #write your optimizer\n",
    "model5.compile(optimizer=optimiser5, loss='categorical_crossentropy', metrics=['categorical_accuracy'])\n",
    "print (model5.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#generator \n",
    "train_generator = generator_5(train_path, train_doc, batch_size)\n",
    "val_generator = generator_5(val_path, val_doc, batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:`period` argument is deprecated. Please use `save_freq` to specify the frequency in number of batches seen.\n"
     ]
    }
   ],
   "source": [
    "model_name = 'model_init' + '_' + str(curr_dt_time).replace(' ','').replace(':','_') + '/'\n",
    "    \n",
    "if not os.path.exists(model_name):\n",
    "    os.mkdir(model_name)\n",
    "        \n",
    "filepath = model_name + 'model-{epoch:05d}-{loss:.5f}-{categorical_accuracy:.5f}-{val_loss:.5f}-{val_categorical_accuracy:.5f}.h5'\n",
    "\n",
    "#Updating checkpoint only for best model because of space constraints.\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='val_loss', verbose=1, save_best_only=False, save_weights_only=False, mode='auto', period=1)\n",
    "\n",
    "LR = ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=2, verbose=1, mode='min', min_delta=0.0001, cooldown=0, min_lr=0.00001) # write the REducelronplateau code here\n",
    "callbacks_list = [checkpoint, LR]\n",
    "#callbacks_list = [LR]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "if (num_train_sequences%batch_size) == 0:\n",
    "    steps_per_epoch = int(num_train_sequences/batch_size)\n",
    "else:\n",
    "    steps_per_epoch = (num_train_sequences//batch_size) + 1\n",
    "\n",
    "if (num_val_sequences%batch_size) == 0:\n",
    "    validation_steps = int(num_val_sequences/batch_size)\n",
    "else:\n",
    "    validation_steps = (num_val_sequences//batch_size) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_375/480912720.py:2: UserWarning: `Model.fit_generator` is deprecated and will be removed in a future version. Please use `Model.fit`, which supports generators.\n",
      "  history = model5.fit_generator(train_generator, steps_per_epoch=steps_per_epoch, epochs=num_epochs, verbose=1,\n",
      "/tmp/ipykernel_375/52623464.py:17: DeprecationWarning:     `imread` is deprecated!\n",
      "    `imread` is deprecated in SciPy 1.0.0, and will be removed in 1.2.0.\n",
      "    Use ``imageio.imread`` instead.\n",
      "  image = imread(source_path+'/'+ t[folder + (batch*batch_size)].strip().split(';')[0]+'/'+imgs[item])\n",
      "/tmp/ipykernel_375/52623464.py:23: DeprecationWarning:     `imresize` is deprecated!\n",
      "    `imresize` is deprecated in SciPy 1.0.0, and will be removed in 1.2.0.\n",
      "    Use ``skimage.transform.resize`` instead.\n",
      "  image = imresize(image[:,20:140,:], (y,z)).astype(np.float32)\n",
      "/tmp/ipykernel_375/52623464.py:25: DeprecationWarning:     `imresize` is deprecated!\n",
      "    `imresize` is deprecated in SciPy 1.0.0, and will be removed in 1.2.0.\n",
      "    Use ``skimage.transform.resize`` instead.\n",
      "  image = imresize(image, (y,z)).astype(np.float32)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Source path =  datasets/Project_data/train ; batch size = 40\n",
      "Epoch 1/35\n",
      "15/17 [=========================>....] - ETA: 3s - loss: 1.5988 - categorical_accuracy: 0.2400"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_375/52623464.py:43: DeprecationWarning:     `imread` is deprecated!\n",
      "    `imread` is deprecated in SciPy 1.0.0, and will be removed in 1.2.0.\n",
      "    Use ``imageio.imread`` instead.\n",
      "  image = imread(source_path+'/'+ t[folder + (num_batches*batch_size)].strip().split(';')[0]+'/'+imgs[item])\n",
      "/tmp/ipykernel_375/52623464.py:47: DeprecationWarning:     `imresize` is deprecated!\n",
      "    `imresize` is deprecated in SciPy 1.0.0, and will be removed in 1.2.0.\n",
      "    Use ``skimage.transform.resize`` instead.\n",
      "  image = imresize(image[:,20:140,:], (y,z)).astype(np.float32)\n",
      "/tmp/ipykernel_375/52623464.py:49: DeprecationWarning:     `imresize` is deprecated!\n",
      "    `imresize` is deprecated in SciPy 1.0.0, and will be removed in 1.2.0.\n",
      "    Use ``skimage.transform.resize`` instead.\n",
      "  image = imresize(image, (y,z)).astype(np.float32)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17/17 [==============================] - ETA: 0s - loss: 1.5953 - categorical_accuracy: 0.2428Source path =  datasets/Project_data/val ; batch size = 40\n",
      "\n",
      "Epoch 00001: saving model to model_init_2022-05-1510_43_57.654621/model-00001-1.59529-0.24284-1.53409-0.28000.h5\n",
      "17/17 [==============================] - 49s 3s/step - loss: 1.5953 - categorical_accuracy: 0.2428 - val_loss: 1.5341 - val_categorical_accuracy: 0.2800 - lr: 0.0100\n",
      "Epoch 2/35\n",
      "17/17 [==============================] - ETA: 0s - loss: 1.4520 - categorical_accuracy: 0.3786\n",
      "Epoch 00002: saving model to model_init_2022-05-1510_43_57.654621/model-00002-1.45199-0.37858-1.39718-0.39000.h5\n",
      "17/17 [==============================] - 42s 3s/step - loss: 1.4520 - categorical_accuracy: 0.3786 - val_loss: 1.3972 - val_categorical_accuracy: 0.3900 - lr: 0.0100\n",
      "Epoch 3/35\n",
      "17/17 [==============================] - ETA: 0s - loss: 1.3126 - categorical_accuracy: 0.4615\n",
      "Epoch 00003: saving model to model_init_2022-05-1510_43_57.654621/model-00003-1.31258-0.46154-1.34002-0.51000.h5\n",
      "17/17 [==============================] - 40s 2s/step - loss: 1.3126 - categorical_accuracy: 0.4615 - val_loss: 1.3400 - val_categorical_accuracy: 0.5100 - lr: 0.0100\n",
      "Epoch 4/35\n",
      "17/17 [==============================] - ETA: 0s - loss: 1.2149 - categorical_accuracy: 0.5083\n",
      "Epoch 00004: saving model to model_init_2022-05-1510_43_57.654621/model-00004-1.21486-0.50830-1.17585-0.63000.h5\n",
      "17/17 [==============================] - 41s 3s/step - loss: 1.2149 - categorical_accuracy: 0.5083 - val_loss: 1.1759 - val_categorical_accuracy: 0.6300 - lr: 0.0100\n",
      "Epoch 5/35\n",
      "17/17 [==============================] - ETA: 0s - loss: 1.0920 - categorical_accuracy: 0.5732\n",
      "Epoch 00005: saving model to model_init_2022-05-1510_43_57.654621/model-00005-1.09198-0.57315-1.08627-0.67000.h5\n",
      "17/17 [==============================] - 41s 3s/step - loss: 1.0920 - categorical_accuracy: 0.5732 - val_loss: 1.0863 - val_categorical_accuracy: 0.6700 - lr: 0.0100\n",
      "Epoch 6/35\n",
      "17/17 [==============================] - ETA: 0s - loss: 0.9886 - categorical_accuracy: 0.6305\n",
      "Epoch 00006: saving model to model_init_2022-05-1510_43_57.654621/model-00006-0.98865-0.63047-0.91681-0.71000.h5\n",
      "17/17 [==============================] - 40s 2s/step - loss: 0.9886 - categorical_accuracy: 0.6305 - val_loss: 0.9168 - val_categorical_accuracy: 0.7100 - lr: 0.0100\n",
      "Epoch 7/35\n",
      "17/17 [==============================] - ETA: 0s - loss: 0.8656 - categorical_accuracy: 0.7270\n",
      "Epoch 00007: saving model to model_init_2022-05-1510_43_57.654621/model-00007-0.86562-0.72700-0.88297-0.69000.h5\n",
      "17/17 [==============================] - 41s 3s/step - loss: 0.8656 - categorical_accuracy: 0.7270 - val_loss: 0.8830 - val_categorical_accuracy: 0.6900 - lr: 0.0100\n",
      "Epoch 8/35\n",
      "17/17 [==============================] - ETA: 0s - loss: 0.7741 - categorical_accuracy: 0.7798\n",
      "Epoch 00008: saving model to model_init_2022-05-1510_43_57.654621/model-00008-0.77408-0.77979-1.00104-0.67000.h5\n",
      "17/17 [==============================] - 42s 3s/step - loss: 0.7741 - categorical_accuracy: 0.7798 - val_loss: 1.0010 - val_categorical_accuracy: 0.6700 - lr: 0.0100\n",
      "Epoch 9/35\n",
      "17/17 [==============================] - ETA: 0s - loss: 0.6869 - categorical_accuracy: 0.7813\n",
      "Epoch 00009: saving model to model_init_2022-05-1510_43_57.654621/model-00009-0.68692-0.78130-0.75839-0.73000.h5\n",
      "17/17 [==============================] - 41s 3s/step - loss: 0.6869 - categorical_accuracy: 0.7813 - val_loss: 0.7584 - val_categorical_accuracy: 0.7300 - lr: 0.0100\n",
      "Epoch 10/35\n",
      "17/17 [==============================] - ETA: 0s - loss: 0.6121 - categorical_accuracy: 0.8250\n",
      "Epoch 00010: saving model to model_init_2022-05-1510_43_57.654621/model-00010-0.61214-0.82504-0.71115-0.69000.h5\n",
      "17/17 [==============================] - 42s 3s/step - loss: 0.6121 - categorical_accuracy: 0.8250 - val_loss: 0.7111 - val_categorical_accuracy: 0.6900 - lr: 0.0100\n",
      "Epoch 11/35\n",
      "17/17 [==============================] - ETA: 0s - loss: 0.5233 - categorical_accuracy: 0.8477\n",
      "Epoch 00011: saving model to model_init_2022-05-1510_43_57.654621/model-00011-0.52325-0.84766-0.61080-0.73000.h5\n",
      "17/17 [==============================] - 41s 3s/step - loss: 0.5233 - categorical_accuracy: 0.8477 - val_loss: 0.6108 - val_categorical_accuracy: 0.7300 - lr: 0.0100\n",
      "Epoch 12/35\n",
      "17/17 [==============================] - ETA: 0s - loss: 0.4600 - categorical_accuracy: 0.8839\n",
      "Epoch 00012: saving model to model_init_2022-05-1510_43_57.654621/model-00012-0.45997-0.88386-0.75340-0.73000.h5\n",
      "17/17 [==============================] - 41s 3s/step - loss: 0.4600 - categorical_accuracy: 0.8839 - val_loss: 0.7534 - val_categorical_accuracy: 0.7300 - lr: 0.0100\n",
      "Epoch 13/35\n",
      "17/17 [==============================] - ETA: 0s - loss: 0.4755 - categorical_accuracy: 0.8658\n",
      "Epoch 00013: saving model to model_init_2022-05-1510_43_57.654621/model-00013-0.47552-0.86576-0.64261-0.76000.h5\n",
      "\n",
      "Epoch 00013: ReduceLROnPlateau reducing learning rate to 0.0009999999776482583.\n",
      "17/17 [==============================] - 42s 3s/step - loss: 0.4755 - categorical_accuracy: 0.8658 - val_loss: 0.6426 - val_categorical_accuracy: 0.7600 - lr: 0.0100\n",
      "Epoch 14/35\n",
      "17/17 [==============================] - ETA: 0s - loss: 0.3545 - categorical_accuracy: 0.9246\n",
      "Epoch 00014: saving model to model_init_2022-05-1510_43_57.654621/model-00014-0.35451-0.92459-0.70611-0.73000.h5\n",
      "17/17 [==============================] - 42s 3s/step - loss: 0.3545 - categorical_accuracy: 0.9246 - val_loss: 0.7061 - val_categorical_accuracy: 0.7300 - lr: 1.0000e-03\n",
      "Epoch 15/35\n",
      "17/17 [==============================] - ETA: 0s - loss: 0.3128 - categorical_accuracy: 0.9427\n",
      "Epoch 00015: saving model to model_init_2022-05-1510_43_57.654621/model-00015-0.31284-0.94268-0.56773-0.78000.h5\n",
      "17/17 [==============================] - 41s 3s/step - loss: 0.3128 - categorical_accuracy: 0.9427 - val_loss: 0.5677 - val_categorical_accuracy: 0.7800 - lr: 1.0000e-03\n",
      "Epoch 16/35\n",
      "17/17 [==============================] - ETA: 0s - loss: 0.3243 - categorical_accuracy: 0.9321\n",
      "Epoch 00016: saving model to model_init_2022-05-1510_43_57.654621/model-00016-0.32433-0.93213-0.61479-0.77000.h5\n",
      "17/17 [==============================] - 42s 3s/step - loss: 0.3243 - categorical_accuracy: 0.9321 - val_loss: 0.6148 - val_categorical_accuracy: 0.7700 - lr: 1.0000e-03\n",
      "Epoch 17/35\n",
      "17/17 [==============================] - ETA: 0s - loss: 0.3159 - categorical_accuracy: 0.9442\n",
      "Epoch 00017: saving model to model_init_2022-05-1510_43_57.654621/model-00017-0.31589-0.94419-0.57763-0.77000.h5\n",
      "\n",
      "Epoch 00017: ReduceLROnPlateau reducing learning rate to 9.999999310821295e-05.\n",
      "17/17 [==============================] - 40s 2s/step - loss: 0.3159 - categorical_accuracy: 0.9442 - val_loss: 0.5776 - val_categorical_accuracy: 0.7700 - lr: 1.0000e-03\n",
      "Epoch 18/35\n",
      "17/17 [==============================] - ETA: 0s - loss: 0.3159 - categorical_accuracy: 0.9442\n",
      "Epoch 00018: saving model to model_init_2022-05-1510_43_57.654621/model-00018-0.31587-0.94419-0.63963-0.75000.h5\n",
      "17/17 [==============================] - 39s 2s/step - loss: 0.3159 - categorical_accuracy: 0.9442 - val_loss: 0.6396 - val_categorical_accuracy: 0.7500 - lr: 1.0000e-04\n",
      "Epoch 19/35\n",
      "17/17 [==============================] - ETA: 0s - loss: 0.3193 - categorical_accuracy: 0.9261\n",
      "Epoch 00019: saving model to model_init_2022-05-1510_43_57.654621/model-00019-0.31935-0.92609-0.61276-0.77000.h5\n",
      "\n",
      "Epoch 00019: ReduceLROnPlateau reducing learning rate to 1e-05.\n",
      "17/17 [==============================] - 39s 2s/step - loss: 0.3193 - categorical_accuracy: 0.9261 - val_loss: 0.6128 - val_categorical_accuracy: 0.7700 - lr: 1.0000e-04\n",
      "Epoch 20/35\n",
      "17/17 [==============================] - ETA: 0s - loss: 0.3023 - categorical_accuracy: 0.9548\n",
      "Epoch 00020: saving model to model_init_2022-05-1510_43_57.654621/model-00020-0.30233-0.95475-0.66840-0.75000.h5\n",
      "17/17 [==============================] - 39s 2s/step - loss: 0.3023 - categorical_accuracy: 0.9548 - val_loss: 0.6684 - val_categorical_accuracy: 0.7500 - lr: 1.0000e-05\n",
      "Epoch 21/35\n",
      "17/17 [==============================] - ETA: 0s - loss: 0.3163 - categorical_accuracy: 0.9397\n",
      "Epoch 00021: saving model to model_init_2022-05-1510_43_57.654621/model-00021-0.31633-0.93967-0.57684-0.80000.h5\n",
      "17/17 [==============================] - 39s 2s/step - loss: 0.3163 - categorical_accuracy: 0.9397 - val_loss: 0.5768 - val_categorical_accuracy: 0.8000 - lr: 1.0000e-05\n",
      "Epoch 22/35\n",
      "17/17 [==============================] - ETA: 0s - loss: 0.2984 - categorical_accuracy: 0.9548\n",
      "Epoch 00022: saving model to model_init_2022-05-1510_43_57.654621/model-00022-0.29835-0.95475-0.61282-0.77000.h5\n",
      "17/17 [==============================] - 41s 3s/step - loss: 0.2984 - categorical_accuracy: 0.9548 - val_loss: 0.6128 - val_categorical_accuracy: 0.7700 - lr: 1.0000e-05\n",
      "Epoch 23/35\n",
      "17/17 [==============================] - ETA: 0s - loss: 0.3102 - categorical_accuracy: 0.9517\n",
      "Epoch 00023: saving model to model_init_2022-05-1510_43_57.654621/model-00023-0.31019-0.95173-0.58293-0.81000.h5\n",
      "17/17 [==============================] - 41s 3s/step - loss: 0.3102 - categorical_accuracy: 0.9517 - val_loss: 0.5829 - val_categorical_accuracy: 0.8100 - lr: 1.0000e-05\n",
      "Epoch 24/35\n",
      "17/17 [==============================] - ETA: 0s - loss: 0.3136 - categorical_accuracy: 0.9502\n",
      "Epoch 00024: saving model to model_init_2022-05-1510_43_57.654621/model-00024-0.31360-0.95023-0.63710-0.76000.h5\n",
      "17/17 [==============================] - 40s 2s/step - loss: 0.3136 - categorical_accuracy: 0.9502 - val_loss: 0.6371 - val_categorical_accuracy: 0.7600 - lr: 1.0000e-05\n",
      "Epoch 25/35\n",
      "17/17 [==============================] - ETA: 0s - loss: 0.2951 - categorical_accuracy: 0.9593\n",
      "Epoch 00025: saving model to model_init_2022-05-1510_43_57.654621/model-00025-0.29506-0.95928-0.61277-0.77000.h5\n",
      "17/17 [==============================] - 40s 3s/step - loss: 0.2951 - categorical_accuracy: 0.9593 - val_loss: 0.6128 - val_categorical_accuracy: 0.7700 - lr: 1.0000e-05\n",
      "Epoch 26/35\n",
      "17/17 [==============================] - ETA: 0s - loss: 0.2985 - categorical_accuracy: 0.9623\n",
      "Epoch 00026: saving model to model_init_2022-05-1510_43_57.654621/model-00026-0.29855-0.96229-0.62513-0.75000.h5\n",
      "17/17 [==============================] - 40s 2s/step - loss: 0.2985 - categorical_accuracy: 0.9623 - val_loss: 0.6251 - val_categorical_accuracy: 0.7500 - lr: 1.0000e-05\n",
      "Epoch 27/35\n",
      "17/17 [==============================] - ETA: 0s - loss: 0.3050 - categorical_accuracy: 0.9412\n",
      "Epoch 00027: saving model to model_init_2022-05-1510_43_57.654621/model-00027-0.30503-0.94118-0.64838-0.76000.h5\n",
      "17/17 [==============================] - 39s 2s/step - loss: 0.3050 - categorical_accuracy: 0.9412 - val_loss: 0.6484 - val_categorical_accuracy: 0.7600 - lr: 1.0000e-05\n",
      "Epoch 28/35\n",
      "17/17 [==============================] - ETA: 0s - loss: 0.3036 - categorical_accuracy: 0.9457\n",
      "Epoch 00028: saving model to model_init_2022-05-1510_43_57.654621/model-00028-0.30364-0.94570-0.61292-0.77000.h5\n",
      "17/17 [==============================] - 42s 3s/step - loss: 0.3036 - categorical_accuracy: 0.9457 - val_loss: 0.6129 - val_categorical_accuracy: 0.7700 - lr: 1.0000e-05\n",
      "Epoch 29/35\n",
      "17/17 [==============================] - ETA: 0s - loss: 0.3015 - categorical_accuracy: 0.9457\n",
      "Epoch 00029: saving model to model_init_2022-05-1510_43_57.654621/model-00029-0.30147-0.94570-0.51498-0.82000.h5\n",
      "17/17 [==============================] - 41s 3s/step - loss: 0.3015 - categorical_accuracy: 0.9457 - val_loss: 0.5150 - val_categorical_accuracy: 0.8200 - lr: 1.0000e-05\n",
      "Epoch 30/35\n",
      "17/17 [==============================] - ETA: 0s - loss: 0.3176 - categorical_accuracy: 0.9397\n",
      "Epoch 00030: saving model to model_init_2022-05-1510_43_57.654621/model-00030-0.31763-0.93967-0.57913-0.78000.h5\n",
      "17/17 [==============================] - 40s 2s/step - loss: 0.3176 - categorical_accuracy: 0.9397 - val_loss: 0.5791 - val_categorical_accuracy: 0.7800 - lr: 1.0000e-05\n",
      "Epoch 31/35\n",
      "17/17 [==============================] - ETA: 0s - loss: 0.3063 - categorical_accuracy: 0.9578\n",
      "Epoch 00031: saving model to model_init_2022-05-1510_43_57.654621/model-00031-0.30627-0.95777-0.61314-0.77000.h5\n",
      "17/17 [==============================] - 41s 3s/step - loss: 0.3063 - categorical_accuracy: 0.9578 - val_loss: 0.6131 - val_categorical_accuracy: 0.7700 - lr: 1.0000e-05\n",
      "Epoch 32/35\n",
      "17/17 [==============================] - ETA: 0s - loss: 0.3224 - categorical_accuracy: 0.9502\n",
      "Epoch 00032: saving model to model_init_2022-05-1510_43_57.654621/model-00032-0.32240-0.95023-0.67592-0.74000.h5\n",
      "17/17 [==============================] - 42s 3s/step - loss: 0.3224 - categorical_accuracy: 0.9502 - val_loss: 0.6759 - val_categorical_accuracy: 0.7400 - lr: 1.0000e-05\n",
      "Epoch 33/35\n",
      "17/17 [==============================] - ETA: 0s - loss: 0.3016 - categorical_accuracy: 0.9472\n",
      "Epoch 00033: saving model to model_init_2022-05-1510_43_57.654621/model-00033-0.30161-0.94721-0.66018-0.75000.h5\n",
      "17/17 [==============================] - 40s 2s/step - loss: 0.3016 - categorical_accuracy: 0.9472 - val_loss: 0.6602 - val_categorical_accuracy: 0.7500 - lr: 1.0000e-05\n",
      "Epoch 34/35\n",
      "17/17 [==============================] - ETA: 0s - loss: 0.3010 - categorical_accuracy: 0.9472\n",
      "Epoch 00034: saving model to model_init_2022-05-1510_43_57.654621/model-00034-0.30097-0.94721-0.61328-0.77000.h5\n",
      "17/17 [==============================] - 41s 3s/step - loss: 0.3010 - categorical_accuracy: 0.9472 - val_loss: 0.6133 - val_categorical_accuracy: 0.7700 - lr: 1.0000e-05\n",
      "Epoch 35/35\n",
      "17/17 [==============================] - ETA: 0s - loss: 0.2950 - categorical_accuracy: 0.9548\n",
      "Epoch 00035: saving model to model_init_2022-05-1510_43_57.654621/model-00035-0.29504-0.95475-0.58066-0.80000.h5\n",
      "17/17 [==============================] - 41s 3s/step - loss: 0.2950 - categorical_accuracy: 0.9548 - val_loss: 0.5807 - val_categorical_accuracy: 0.8000 - lr: 1.0000e-05\n"
     ]
    }
   ],
   "source": [
    "#Fitting the model.\n",
    "history = model5.fit_generator(train_generator, steps_per_epoch=steps_per_epoch, epochs=num_epochs, verbose=1, \n",
    "                    callbacks=callbacks_list, validation_data=val_generator, \n",
    "                    validation_steps=validation_steps, class_weight=None, workers=1, initial_epoch=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 576x576 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "acc = history.history['categorical_accuracy']\n",
    "val_acc = history.history['val_categorical_accuracy']\n",
    "\n",
    "loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']\n",
    "\n",
    "epochs_range = range(num_epochs)\n",
    "\n",
    "plt.figure(figsize=(8, 8))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(epochs_range, acc, label='Training Accuracy')\n",
    "plt.plot(epochs_range, val_acc, label='Validation Accuracy')\n",
    "plt.legend(loc='lower right')\n",
    "plt.title('Training and Validation Accuracy')\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(epochs_range, loss, label='Training Loss')\n",
    "plt.plot(epochs_range, val_loss, label='Validation Loss')\n",
    "plt.legend(loc='upper right')\n",
    "plt.title('Training and Validation Loss')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model 4:  mobilenet + gru Adam optimizer not training all layers , epoch 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.applications import mobilenet\n",
    "import abc\n",
    "from keras.layers.convolutional import Conv3D, MaxPooling3D, Conv2D, MaxPooling2D\n",
    "from keras.layers.recurrent import LSTM\n",
    "import cv2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModelBuilder(metaclass= abc.ABCMeta):\n",
    "    \n",
    "    def initialize_path(self,project_folder):\n",
    "        self.train_doc = np.random.permutation(open(project_folder + '/' + 'train.csv').readlines())\n",
    "        self.val_doc = np.random.permutation(open(project_folder + '/' + 'val.csv').readlines())\n",
    "        self.train_path = project_folder + '/' + 'train'\n",
    "        self.val_path =  project_folder + '/' + 'val'\n",
    "        self.num_train_sequences = len(self.train_doc)\n",
    "        self.num_val_sequences = len(self.val_doc)\n",
    "        \n",
    "    def initialize_image_properties(self,image_height=100,image_width=100):\n",
    "        self.image_height=image_height\n",
    "        self.image_width=image_width\n",
    "        self.channels=3\n",
    "        self.num_classes=5\n",
    "        self.total_frames=30\n",
    "          \n",
    "    def initialize_hyperparams(self,frames_to_sample=30,batch_size=20,num_epochs=20):\n",
    "        self.frames_to_sample=frames_to_sample\n",
    "        self.batch_size=batch_size\n",
    "        self.num_epochs=num_epochs\n",
    "        \n",
    "        \n",
    "    def generator(self,source_path, folder_list, augment=False):\n",
    "        img_idx = np.round(np.linspace(0,self.total_frames-1,self.frames_to_sample)).astype(int)\n",
    "        batch_size=self.batch_size\n",
    "        while True:\n",
    "            t = np.random.permutation(folder_list)\n",
    "            num_batches = len(t)//batch_size\n",
    "        \n",
    "            for batch in range(num_batches): \n",
    "                batch_data, batch_labels= self.one_batch_data(source_path,t,batch,batch_size,img_idx,augment)\n",
    "                yield batch_data, batch_labels \n",
    "\n",
    "            remaining_seq=len(t)%batch_size\n",
    "        \n",
    "            if (remaining_seq != 0):\n",
    "                batch_data, batch_labels= self.one_batch_data(source_path,t,num_batches,batch_size,img_idx,augment,remaining_seq)\n",
    "                yield batch_data, batch_labels \n",
    "    \n",
    "    \n",
    "    def one_batch_data(self,source_path,t,batch,batch_size,img_idx,augment,remaining_seq=0):\n",
    "    \n",
    "        seq_len = remaining_seq if remaining_seq else batch_size\n",
    "    \n",
    "        batch_data = np.zeros((seq_len,len(img_idx),self.image_height,self.image_width,self.channels)) \n",
    "        batch_labels = np.zeros((seq_len,self.num_classes)) \n",
    "    \n",
    "        if (augment): batch_data_aug = np.zeros((seq_len,len(img_idx),self.image_height,self.image_width,self.channels))\n",
    "        for folder in range(seq_len): \n",
    "            imgs = os.listdir(source_path+'/'+ t[folder + (batch*batch_size)].split(';')[0]) \n",
    "            for idx,item in enumerate(img_idx): \n",
    "                image = imread(source_path+'/'+ t[folder + (batch*batch_size)].strip().split(';')[0]+'/'+imgs[item]).astype(np.float32)\n",
    "                image_resized=imresize(image,(self.image_height,self.image_width,3))\n",
    "            \n",
    "\n",
    "                batch_data[folder,idx,:,:,0] = (image_resized[:,:,0])/255\n",
    "                batch_data[folder,idx,:,:,1] = (image_resized[:,:,1])/255\n",
    "                batch_data[folder,idx,:,:,2] = (image_resized[:,:,2])/255\n",
    "            \n",
    "                if (augment):\n",
    "                    shifted = cv2.warpAffine(image, \n",
    "                                             np.float32([[1, 0, np.random.randint(-30,30)],[0, 1, np.random.randint(-30,30)]]), \n",
    "                                            (image.shape[1], image.shape[0]))\n",
    "                    \n",
    "                    gray = cv2.cvtColor(shifted,cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "                    x0, y0 = np.argwhere(gray > 0).min(axis=0)\n",
    "                    x1, y1 = np.argwhere(gray > 0).max(axis=0) \n",
    "                    \n",
    "                    cropped=shifted[x0:x1,y0:y1,:]\n",
    "                    \n",
    "                    image_resized=imresize(cropped,(self.image_height,self.image_width,3))\n",
    "                    \n",
    "                    #shifted = cv2.warpAffine(image_resized, \n",
    "                    #                        np.float32([[1, 0, np.random.randint(-3,3)],[0, 1, np.random.randint(-3,3)]]), \n",
    "                    #                        (image_resized.shape[1], image_resized.shape[0]))\n",
    "            \n",
    "                    batch_data_aug[folder,idx,:,:,0] = (image_resized[:,:,0])/255\n",
    "                    batch_data_aug[folder,idx,:,:,1] = (image_resized[:,:,1])/255\n",
    "                    batch_data_aug[folder,idx,:,:,2] = (image_resized[:,:,2])/255\n",
    "                \n",
    "            \n",
    "            batch_labels[folder, int(t[folder + (batch*batch_size)].strip().split(';')[2])] = 1\n",
    "            \n",
    "    \n",
    "        if (augment):\n",
    "            batch_data=np.concatenate([batch_data,batch_data_aug])\n",
    "            batch_labels=np.concatenate([batch_labels,batch_labels])\n",
    "\n",
    "        \n",
    "        return(batch_data,batch_labels)\n",
    "\n",
    "    def train_model(self, model, augment_data=False):\n",
    "        train_generator = self.generator(self.train_path, self.train_doc,augment=augment_data)\n",
    "        val_generator = self.generator(self.val_path, self.val_doc)\n",
    "\n",
    "        model_name = 'model_init' + '_' + str(datetime.datetime.now()).replace(' ','').replace(':','_') + '/'\n",
    "    \n",
    "        if not os.path.exists(model_name):\n",
    "            os.mkdir(model_name)\n",
    "        \n",
    "        filepath = model_name + 'model-{epoch:05d}-{loss:.5f}-{categorical_accuracy:.5f}-{val_loss:.5f}-{val_categorical_accuracy:.5f}.h5'\n",
    "\n",
    "        checkpoint = ModelCheckpoint(filepath, monitor='val_loss', verbose=1, save_best_only=False, save_weights_only=False, mode='auto', period=1)\n",
    "        LR = ReduceLROnPlateau(monitor='val_loss', factor=0.2, verbose=1, patience=4)\n",
    "        callbacks_list = [checkpoint, LR]\n",
    "\n",
    "        if (self.num_train_sequences%self.batch_size) == 0:\n",
    "            steps_per_epoch = int(self.num_train_sequences/self.batch_size)\n",
    "        else:\n",
    "            steps_per_epoch = (self.num_train_sequences//self.batch_size) + 1\n",
    "\n",
    "        if (self.num_val_sequences%self.batch_size) == 0:\n",
    "            validation_steps = int(self.num_val_sequences/self.batch_size)\n",
    "        else:\n",
    "            validation_steps = (self.num_val_sequences//self.batch_size) + 1\n",
    "    \n",
    "        history=model.fit_generator(train_generator, steps_per_epoch=steps_per_epoch, epochs=self.num_epochs, verbose=1, \n",
    "                            callbacks=callbacks_list, validation_data=val_generator, \n",
    "                            validation_steps=validation_steps, class_weight=None, workers=1, initial_epoch=0)\n",
    "        return history\n",
    "\n",
    "        \n",
    "    @abc.abstractmethod\n",
    "    def define_model(self):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModelBuilderMoreAugmentation(metaclass= abc.ABCMeta):\n",
    "    \n",
    "    def initialize_path(self,project_folder):\n",
    "        self.train_doc = np.random.permutation(open(project_folder + '/' + 'train.csv').readlines())\n",
    "        self.val_doc = np.random.permutation(open(project_folder + '/' + 'val.csv').readlines())\n",
    "        self.train_path = project_folder + '/' + 'train'\n",
    "        self.val_path =  project_folder + '/' + 'val'\n",
    "        self.num_train_sequences = len(self.train_doc)\n",
    "        self.num_val_sequences = len(self.val_doc)\n",
    "        \n",
    "    def initialize_image_properties(self,image_height=100,image_width=100):\n",
    "        self.image_height=image_height\n",
    "        self.image_width=image_width\n",
    "        self.channels=3\n",
    "        self.num_classes=5\n",
    "        self.total_frames=30\n",
    "          \n",
    "    def initialize_hyperparams(self,frames_to_sample=30,batch_size=20,num_epochs=20):\n",
    "        self.frames_to_sample=frames_to_sample\n",
    "        self.batch_size=batch_size\n",
    "        self.num_epochs=num_epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:`input_shape` is undefined or non-square, or `rows` is not in [128, 160, 192, 224]. Weights for input shape (224, 224) will be loaded as the default.\n"
     ]
    }
   ],
   "source": [
    "mobilenet_transfer = mobilenet.MobileNet(weights='imagenet', include_top=False)\n",
    "\n",
    "class RNNCNN_TL(ModelBuilderMoreAugmentation):\n",
    "    \n",
    "    def define_model(self,lstm_cells=64,dense_neurons=64,dropout=0.25):\n",
    "        \n",
    "        model = Sequential()\n",
    "        model.add(TimeDistributed(mobilenet_transfer,input_shape=(self.frames_to_sample,self.image_height,self.image_width,self.channels)))\n",
    "        \n",
    "        \n",
    "        for layer in model.layers:\n",
    "            layer.trainable = False\n",
    "        \n",
    "        \n",
    "        model.add(TimeDistributed(BatchNormalization()))\n",
    "        model.add(TimeDistributed(MaxPooling2D((2, 2))))\n",
    "        model.add(TimeDistributed(Flatten()))\n",
    "\n",
    "        model.add(LSTM(lstm_cells))\n",
    "        model.add(Dropout(dropout))\n",
    "        \n",
    "        model.add(Dense(dense_neurons,activation='relu'))\n",
    "        model.add(Dropout(dropout))\n",
    "        \n",
    "        model.add(Dense(self.num_classes, activation='softmax'))\n",
    "        \n",
    "        \n",
    "        optimiser ='adam' #optimizers.Adam()\n",
    "        model.compile(optimizer=optimiser, loss='categorical_crossentropy', metrics=['categorical_accuracy'])\n",
    "        return model\n",
    "    def train_model(self, model, augment_data=False):\n",
    "        train_generator = self.generator(self.train_path, self.train_doc,augment=augment_data)\n",
    "        val_generator = self.generator(self.val_path, self.val_doc)\n",
    "\n",
    "        model_name = 'model_init' + '_' + str(datetime.datetime.now()).replace(' ','').replace(':','_') + '/'\n",
    "    \n",
    "        if not os.path.exists(model_name):\n",
    "            os.mkdir(model_name)\n",
    "        \n",
    "        filepath = model_name + 'model-{epoch:05d}-{loss:.5f}-{categorical_accuracy:.5f}-{val_loss:.5f}-{val_categorical_accuracy:.5f}.h5'\n",
    "\n",
    "        checkpoint = ModelCheckpoint(filepath, monitor='val_loss', verbose=1, save_best_only=False, save_weights_only=False, mode='auto', period=1)\n",
    "        LR = ReduceLROnPlateau(monitor='val_loss', factor=0.2, verbose=1, patience=4)\n",
    "        callbacks_list = [checkpoint, LR]\n",
    "\n",
    "        if (self.num_train_sequences%self.batch_size) == 0:\n",
    "            steps_per_epoch = int(self.num_train_sequences/self.batch_size)\n",
    "        else:\n",
    "            steps_per_epoch = (self.num_train_sequences//self.batch_size) + 1\n",
    "\n",
    "        if (self.num_val_sequences%self.batch_size) == 0:\n",
    "            validation_steps = int(self.num_val_sequences/self.batch_size)\n",
    "        else:\n",
    "            validation_steps = (self.num_val_sequences//self.batch_size) + 1\n",
    "    \n",
    "        history=model.fit_generator(train_generator, steps_per_epoch=steps_per_epoch, epochs=self.num_epochs, verbose=1, \n",
    "                            callbacks=callbacks_list, validation_data=val_generator, \n",
    "                            validation_steps=validation_steps, class_weight=None, workers=1, initial_epoch=0)\n",
    "        return history\n",
    "    def initialize_path(self,project_folder):\n",
    "        self.train_doc = np.random.permutation(open(project_folder + '/' + 'train.csv').readlines())\n",
    "        self.val_doc = np.random.permutation(open(project_folder + '/' + 'val.csv').readlines())\n",
    "        self.train_path = project_folder + '/' + 'train'\n",
    "        self.val_path =  project_folder + '/' + 'val'\n",
    "        self.num_train_sequences = len(self.train_doc)\n",
    "        self.num_val_sequences = len(self.val_doc)\n",
    "        \n",
    "    def initialize_image_properties(self,image_height=100,image_width=100):\n",
    "        self.image_height=image_height\n",
    "        self.image_width=image_width\n",
    "        self.channels=3\n",
    "        self.num_classes=5\n",
    "        self.total_frames=30\n",
    "          \n",
    "    def initialize_hyperparams(self,frames_to_sample=30,batch_size=20,num_epochs=20):\n",
    "        self.frames_to_sample=frames_to_sample\n",
    "        self.batch_size=batch_size\n",
    "        self.num_epochs=num_epochs\n",
    "        \n",
    "        \n",
    "    def generator(self,source_path, folder_list, augment=False):\n",
    "        img_idx = np.round(np.linspace(0,self.total_frames-1,self.frames_to_sample)).astype(int)\n",
    "        batch_size=self.batch_size\n",
    "        while True:\n",
    "            t = np.random.permutation(folder_list)\n",
    "            num_batches = len(t)//batch_size\n",
    "        \n",
    "            for batch in range(num_batches): \n",
    "                batch_data, batch_labels= self.one_batch_data(source_path,t,batch,batch_size,img_idx,augment)\n",
    "                yield batch_data, batch_labels \n",
    "\n",
    "            remaining_seq=len(t)%batch_size\n",
    "        \n",
    "            if (remaining_seq != 0):\n",
    "                batch_data, batch_labels= self.one_batch_data(source_path,t,num_batches,batch_size,img_idx,augment,remaining_seq)\n",
    "                yield batch_data, batch_labels \n",
    "    \n",
    "    \n",
    "    def one_batch_data(self,source_path,t,batch,batch_size,img_idx,augment,remaining_seq=0):\n",
    "    \n",
    "        seq_len = remaining_seq if remaining_seq else batch_size\n",
    "    \n",
    "        batch_data = np.zeros((seq_len,len(img_idx),self.image_height,self.image_width,self.channels)) \n",
    "        batch_labels = np.zeros((seq_len,self.num_classes)) \n",
    "    \n",
    "        if (augment): batch_data_aug = np.zeros((seq_len,len(img_idx),self.image_height,self.image_width,self.channels))\n",
    "        for folder in range(seq_len): \n",
    "            imgs = os.listdir(source_path+'/'+ t[folder + (batch*batch_size)].split(';')[0]) \n",
    "            for idx,item in enumerate(img_idx): \n",
    "                image = imread(source_path+'/'+ t[folder + (batch*batch_size)].strip().split(';')[0]+'/'+imgs[item]).astype(np.float32)\n",
    "                image_resized=imresize(image,(self.image_height,self.image_width,3))\n",
    "            \n",
    "\n",
    "                batch_data[folder,idx,:,:,0] = (image_resized[:,:,0])/255\n",
    "                batch_data[folder,idx,:,:,1] = (image_resized[:,:,1])/255\n",
    "                batch_data[folder,idx,:,:,2] = (image_resized[:,:,2])/255\n",
    "            \n",
    "                if (augment):\n",
    "                    shifted = cv2.warpAffine(image, \n",
    "                                             np.float32([[1, 0, np.random.randint(-30,30)],[0, 1, np.random.randint(-30,30)]]), \n",
    "                                            (image.shape[1], image.shape[0]))\n",
    "                    \n",
    "                    gray = cv2.cvtColor(shifted,cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "                    x0, y0 = np.argwhere(gray > 0).min(axis=0)\n",
    "                    x1, y1 = np.argwhere(gray > 0).max(axis=0) \n",
    "                    \n",
    "                    cropped=shifted[x0:x1,y0:y1,:]\n",
    "                    \n",
    "                    image_resized=imresize(cropped,(self.image_height,self.image_width,3))\n",
    "                    \n",
    "                    #shifted = cv2.warpAffine(image_resized, \n",
    "                    #                        np.float32([[1, 0, np.random.randint(-3,3)],[0, 1, np.random.randint(-3,3)]]), \n",
    "                    #                        (image_resized.shape[1], image_resized.shape[0]))\n",
    "            \n",
    "                    batch_data_aug[folder,idx,:,:,0] = (image_resized[:,:,0])/255\n",
    "                    batch_data_aug[folder,idx,:,:,1] = (image_resized[:,:,1])/255\n",
    "                    batch_data_aug[folder,idx,:,:,2] = (image_resized[:,:,2])/255\n",
    "                \n",
    "            \n",
    "            batch_labels[folder, int(t[folder + (batch*batch_size)].strip().split(';')[2])] = 1\n",
    "            \n",
    "    \n",
    "        if (augment):\n",
    "            batch_data=np.concatenate([batch_data,batch_data_aug])\n",
    "            batch_labels=np.concatenate([batch_labels,batch_labels])\n",
    "\n",
    "        \n",
    "        return(batch_data,batch_labels)\n",
    "\n",
    "    def train_model(self, model, augment_data=False):\n",
    "        train_generator = self.generator(self.train_path, self.train_doc,augment=augment_data)\n",
    "        val_generator = self.generator(self.val_path, self.val_doc)\n",
    "\n",
    "        model_name = 'model_init' + '_' + str(datetime.datetime.now()).replace(' ','').replace(':','_') + '/'\n",
    "    \n",
    "        if not os.path.exists(model_name):\n",
    "            os.mkdir(model_name)\n",
    "        \n",
    "        filepath = model_name + 'model-{epoch:05d}-{loss:.5f}-{categorical_accuracy:.5f}-{val_loss:.5f}-{val_categorical_accuracy:.5f}.h5'\n",
    "\n",
    "        checkpoint = ModelCheckpoint(filepath, monitor='val_loss', verbose=1, save_best_only=False, save_weights_only=False, mode='auto', period=1)\n",
    "        LR = ReduceLROnPlateau(monitor='val_loss', factor=0.2, verbose=1, patience=4)\n",
    "        callbacks_list = [checkpoint, LR]\n",
    "\n",
    "        if (self.num_train_sequences%self.batch_size) == 0:\n",
    "            steps_per_epoch = int(self.num_train_sequences/self.batch_size)\n",
    "        else:\n",
    "            steps_per_epoch = (self.num_train_sequences//self.batch_size) + 1\n",
    "\n",
    "        if (self.num_val_sequences%self.batch_size) == 0:\n",
    "            validation_steps = int(self.num_val_sequences/self.batch_size)\n",
    "        else:\n",
    "            validation_steps = (self.num_val_sequences//self.batch_size) + 1\n",
    "    \n",
    "        history=model.fit_generator(train_generator, steps_per_epoch=steps_per_epoch, epochs=self.num_epochs, verbose=1, \n",
    "                            callbacks=callbacks_list, validation_data=val_generator, \n",
    "                            validation_steps=validation_steps, class_weight=None, workers=1, initial_epoch=0)\n",
    "        return history\n",
    "\n",
    "        \n",
    "#     @abc.abstractmethod\n",
    "#     def define_model(self):\n",
    "#         pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_3\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " time_distributed_1 (TimeDis  (None, 16, 3, 3, 1024)   3228864   \n",
      " tributed)                                                       \n",
      "                                                                 \n",
      " time_distributed_2 (TimeDis  (None, 16, 3, 3, 1024)   4096      \n",
      " tributed)                                                       \n",
      "                                                                 \n",
      " time_distributed_3 (TimeDis  (None, 16, 1, 1, 1024)   0         \n",
      " tributed)                                                       \n",
      "                                                                 \n",
      " time_distributed_4 (TimeDis  (None, 16, 1024)         0         \n",
      " tributed)                                                       \n",
      "                                                                 \n",
      " lstm (LSTM)                 (None, 128)               590336    \n",
      "                                                                 \n",
      " dropout_5 (Dropout)         (None, 128)               0         \n",
      "                                                                 \n",
      " dense_8 (Dense)             (None, 128)               16512     \n",
      "                                                                 \n",
      " dropout_6 (Dropout)         (None, 128)               0         \n",
      "                                                                 \n",
      " dense_9 (Dense)             (None, 5)                 645       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 3,840,453\n",
      "Trainable params: 609,541\n",
      "Non-trainable params: 3,230,912\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "project_folder='datasets/Project_data'\n",
    "rnn_cnn_tl=RNNCNN_TL()\n",
    "rnn_cnn_tl.initialize_path(project_folder)\n",
    "rnn_cnn_tl.initialize_image_properties(image_height=120,image_width=120)\n",
    "rnn_cnn_tl.initialize_hyperparams(frames_to_sample=16,batch_size=5,num_epochs=20)\n",
    "rnn_cnn_tl_model=rnn_cnn_tl.define_model(lstm_cells=128,dense_neurons=128,dropout=0.25)\n",
    "rnn_cnn_tl_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Params: 3840453\n",
      "WARNING:tensorflow:`period` argument is deprecated. Please use `save_freq` to specify the frequency in number of batches seen.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_375/3671313463.py:176: UserWarning: `Model.fit_generator` is deprecated and will be removed in a future version. Please use `Model.fit`, which supports generators.\n",
      "  history=model.fit_generator(train_generator, steps_per_epoch=steps_per_epoch, epochs=self.num_epochs, verbose=1,\n",
      "/tmp/ipykernel_375/3671313463.py:110: DeprecationWarning:     `imread` is deprecated!\n",
      "    `imread` is deprecated in SciPy 1.0.0, and will be removed in 1.2.0.\n",
      "    Use ``imageio.imread`` instead.\n",
      "  image = imread(source_path+'/'+ t[folder + (batch*batch_size)].strip().split(';')[0]+'/'+imgs[item]).astype(np.float32)\n",
      "/tmp/ipykernel_375/3671313463.py:111: DeprecationWarning:     `imresize` is deprecated!\n",
      "    `imresize` is deprecated in SciPy 1.0.0, and will be removed in 1.2.0.\n",
      "    Use ``skimage.transform.resize`` instead.\n",
      "  image_resized=imresize(image,(self.image_height,self.image_width,3))\n",
      "/tmp/ipykernel_375/3671313463.py:130: DeprecationWarning:     `imresize` is deprecated!\n",
      "    `imresize` is deprecated in SciPy 1.0.0, and will be removed in 1.2.0.\n",
      "    Use ``skimage.transform.resize`` instead.\n",
      "  image_resized=imresize(cropped,(self.image_height,self.image_width,3))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "133/133 [==============================] - ETA: 0s - loss: 1.2533 - categorical_accuracy: 0.4925\n",
      "Epoch 00001: saving model to model_init_2022-05-1511_17_24.011494/model-00001-1.25333-0.49246-0.72099-0.73000.h5\n",
      "133/133 [==============================] - 91s 659ms/step - loss: 1.2533 - categorical_accuracy: 0.4925 - val_loss: 0.7210 - val_categorical_accuracy: 0.7300 - lr: 0.0010\n",
      "Epoch 2/20\n",
      "133/133 [==============================] - ETA: 0s - loss: 0.6597 - categorical_accuracy: 0.7715\n",
      "Epoch 00002: saving model to model_init_2022-05-1511_17_24.011494/model-00002-0.65972-0.77149-0.60658-0.75000.h5\n",
      "133/133 [==============================] - 91s 691ms/step - loss: 0.6597 - categorical_accuracy: 0.7715 - val_loss: 0.6066 - val_categorical_accuracy: 0.7500 - lr: 0.0010\n",
      "Epoch 3/20\n",
      "133/133 [==============================] - ETA: 0s - loss: 0.4412 - categorical_accuracy: 0.8386\n",
      "Epoch 00003: saving model to model_init_2022-05-1511_17_24.011494/model-00003-0.44116-0.83861-0.59711-0.79000.h5\n",
      "133/133 [==============================] - 91s 686ms/step - loss: 0.4412 - categorical_accuracy: 0.8386 - val_loss: 0.5971 - val_categorical_accuracy: 0.7900 - lr: 0.0010\n",
      "Epoch 4/20\n",
      "133/133 [==============================] - ETA: 0s - loss: 0.2627 - categorical_accuracy: 0.9163\n",
      "Epoch 00004: saving model to model_init_2022-05-1511_17_24.011494/model-00004-0.26267-0.91629-0.60866-0.82000.h5\n",
      "133/133 [==============================] - 88s 667ms/step - loss: 0.2627 - categorical_accuracy: 0.9163 - val_loss: 0.6087 - val_categorical_accuracy: 0.8200 - lr: 0.0010\n",
      "Epoch 5/20\n",
      "133/133 [==============================] - ETA: 0s - loss: 0.2576 - categorical_accuracy: 0.9050\n",
      "Epoch 00005: saving model to model_init_2022-05-1511_17_24.011494/model-00005-0.25757-0.90498-0.55578-0.82000.h5\n",
      "133/133 [==============================] - 87s 660ms/step - loss: 0.2576 - categorical_accuracy: 0.9050 - val_loss: 0.5558 - val_categorical_accuracy: 0.8200 - lr: 0.0010\n",
      "Epoch 6/20\n",
      "133/133 [==============================] - ETA: 0s - loss: 0.2374 - categorical_accuracy: 0.9208\n",
      "Epoch 00006: saving model to model_init_2022-05-1511_17_24.011494/model-00006-0.23740-0.92081-0.56585-0.81000.h5\n",
      "133/133 [==============================] - 87s 662ms/step - loss: 0.2374 - categorical_accuracy: 0.9208 - val_loss: 0.5658 - val_categorical_accuracy: 0.8100 - lr: 0.0010\n",
      "Epoch 7/20\n",
      "133/133 [==============================] - ETA: 0s - loss: 0.1247 - categorical_accuracy: 0.9585\n",
      "Epoch 00007: saving model to model_init_2022-05-1511_17_24.011494/model-00007-0.12474-0.95852-0.68742-0.76000.h5\n",
      "133/133 [==============================] - 86s 651ms/step - loss: 0.1247 - categorical_accuracy: 0.9585 - val_loss: 0.6874 - val_categorical_accuracy: 0.7600 - lr: 0.0010\n",
      "Epoch 8/20\n",
      "133/133 [==============================] - ETA: 0s - loss: 0.1514 - categorical_accuracy: 0.9525\n",
      "Epoch 00008: saving model to model_init_2022-05-1511_17_24.011494/model-00008-0.15141-0.95249-0.47241-0.84000.h5\n",
      "133/133 [==============================] - 86s 649ms/step - loss: 0.1514 - categorical_accuracy: 0.9525 - val_loss: 0.4724 - val_categorical_accuracy: 0.8400 - lr: 0.0010\n",
      "Epoch 9/20\n",
      "133/133 [==============================] - ETA: 0s - loss: 0.1194 - categorical_accuracy: 0.9676\n",
      "Epoch 00009: saving model to model_init_2022-05-1511_17_24.011494/model-00009-0.11937-0.96757-0.71497-0.78000.h5\n",
      "133/133 [==============================] - 87s 656ms/step - loss: 0.1194 - categorical_accuracy: 0.9676 - val_loss: 0.7150 - val_categorical_accuracy: 0.7800 - lr: 0.0010\n",
      "Epoch 10/20\n",
      "133/133 [==============================] - ETA: 0s - loss: 0.0812 - categorical_accuracy: 0.9729\n",
      "Epoch 00010: saving model to model_init_2022-05-1511_17_24.011494/model-00010-0.08121-0.97285-0.76174-0.79000.h5\n",
      "133/133 [==============================] - 88s 666ms/step - loss: 0.0812 - categorical_accuracy: 0.9729 - val_loss: 0.7617 - val_categorical_accuracy: 0.7900 - lr: 0.0010\n",
      "Epoch 11/20\n",
      "133/133 [==============================] - ETA: 0s - loss: 0.0761 - categorical_accuracy: 0.9713\n",
      "Epoch 00011: saving model to model_init_2022-05-1511_17_24.011494/model-00011-0.07614-0.97134-0.71602-0.79000.h5\n",
      "133/133 [==============================] - 87s 659ms/step - loss: 0.0761 - categorical_accuracy: 0.9713 - val_loss: 0.7160 - val_categorical_accuracy: 0.7900 - lr: 0.0010\n",
      "Epoch 12/20\n",
      "133/133 [==============================] - ETA: 0s - loss: 0.1141 - categorical_accuracy: 0.9638\n",
      "Epoch 00012: saving model to model_init_2022-05-1511_17_24.011494/model-00012-0.11409-0.96380-0.37387-0.90000.h5\n",
      "133/133 [==============================] - 85s 643ms/step - loss: 0.1141 - categorical_accuracy: 0.9638 - val_loss: 0.3739 - val_categorical_accuracy: 0.9000 - lr: 0.0010\n",
      "Epoch 13/20\n",
      "133/133 [==============================] - ETA: 0s - loss: 0.0742 - categorical_accuracy: 0.9744\n",
      "Epoch 00013: saving model to model_init_2022-05-1511_17_24.011494/model-00013-0.07415-0.97436-0.78062-0.80000.h5\n",
      "133/133 [==============================] - 89s 674ms/step - loss: 0.0742 - categorical_accuracy: 0.9744 - val_loss: 0.7806 - val_categorical_accuracy: 0.8000 - lr: 0.0010\n",
      "Epoch 14/20\n",
      "133/133 [==============================] - ETA: 0s - loss: 0.1165 - categorical_accuracy: 0.9630\n",
      "Epoch 00014: saving model to model_init_2022-05-1511_17_24.011494/model-00014-0.11648-0.96305-0.81647-0.85000.h5\n",
      "133/133 [==============================] - 86s 649ms/step - loss: 0.1165 - categorical_accuracy: 0.9630 - val_loss: 0.8165 - val_categorical_accuracy: 0.8500 - lr: 0.0010\n",
      "Epoch 15/20\n",
      "133/133 [==============================] - ETA: 0s - loss: 0.1035 - categorical_accuracy: 0.9661\n",
      "Epoch 00015: saving model to model_init_2022-05-1511_17_24.011494/model-00015-0.10345-0.96606-0.79795-0.78000.h5\n",
      "133/133 [==============================] - 90s 683ms/step - loss: 0.1035 - categorical_accuracy: 0.9661 - val_loss: 0.7979 - val_categorical_accuracy: 0.7800 - lr: 0.0010\n",
      "Epoch 16/20\n",
      "133/133 [==============================] - ETA: 0s - loss: 0.0757 - categorical_accuracy: 0.9706\n",
      "Epoch 00016: saving model to model_init_2022-05-1511_17_24.011494/model-00016-0.07570-0.97059-0.55669-0.83000.h5\n",
      "\n",
      "Epoch 00016: ReduceLROnPlateau reducing learning rate to 0.00020000000949949026.\n",
      "133/133 [==============================] - 90s 680ms/step - loss: 0.0757 - categorical_accuracy: 0.9706 - val_loss: 0.5567 - val_categorical_accuracy: 0.8300 - lr: 0.0010\n",
      "Epoch 17/20\n",
      "133/133 [==============================] - ETA: 0s - loss: 0.0550 - categorical_accuracy: 0.9857\n",
      "Epoch 00017: saving model to model_init_2022-05-1511_17_24.011494/model-00017-0.05495-0.98567-0.64676-0.78000.h5\n",
      "133/133 [==============================] - 89s 676ms/step - loss: 0.0550 - categorical_accuracy: 0.9857 - val_loss: 0.6468 - val_categorical_accuracy: 0.7800 - lr: 2.0000e-04\n",
      "Epoch 18/20\n",
      "133/133 [==============================] - ETA: 0s - loss: 0.0413 - categorical_accuracy: 0.9857\n",
      "Epoch 00018: saving model to model_init_2022-05-1511_17_24.011494/model-00018-0.04132-0.98567-0.94895-0.78000.h5\n",
      "133/133 [==============================] - 88s 668ms/step - loss: 0.0413 - categorical_accuracy: 0.9857 - val_loss: 0.9490 - val_categorical_accuracy: 0.7800 - lr: 2.0000e-04\n",
      "Epoch 19/20\n",
      "133/133 [==============================] - ETA: 0s - loss: 0.0319 - categorical_accuracy: 0.9894\n",
      "Epoch 00019: saving model to model_init_2022-05-1511_17_24.011494/model-00019-0.03187-0.98944-0.75663-0.80000.h5\n",
      "133/133 [==============================] - 88s 664ms/step - loss: 0.0319 - categorical_accuracy: 0.9894 - val_loss: 0.7566 - val_categorical_accuracy: 0.8000 - lr: 2.0000e-04\n",
      "Epoch 20/20\n",
      "133/133 [==============================] - ETA: 0s - loss: 0.0306 - categorical_accuracy: 0.9879\n",
      "Epoch 00020: saving model to model_init_2022-05-1511_17_24.011494/model-00020-0.03061-0.98793-0.63678-0.84000.h5\n",
      "\n",
      "Epoch 00020: ReduceLROnPlateau reducing learning rate to 4.0000001899898055e-05.\n",
      "133/133 [==============================] - 88s 666ms/step - loss: 0.0306 - categorical_accuracy: 0.9879 - val_loss: 0.6368 - val_categorical_accuracy: 0.8400 - lr: 2.0000e-04\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(\"Total Params:\", rnn_cnn_tl_model.count_params())\n",
    "history_model18=rnn_cnn_tl.train_model(rnn_cnn_tl_model,augment_data=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model 5: Mobilenet 2D + GRU(batch size 5, frames to sample 16) Adam optimizer and training weights for all layers epoch 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:`input_shape` is undefined or non-square, or `rows` is not in [128, 160, 192, 224]. Weights for input shape (224, 224) will be loaded as the default.\n"
     ]
    }
   ],
   "source": [
    "from keras.applications import mobilenet\n",
    "\n",
    "mobilenet_transfer = mobilenet.MobileNet(weights='imagenet', include_top=False)\n",
    "\n",
    "class RNNCNN_TL2(ModelBuilderMoreAugmentation):\n",
    "    \n",
    "    def define_model(self,gru_cells=64,dense_neurons=64,dropout=0.25):\n",
    "        \n",
    "        model = Sequential()\n",
    "        model.add(TimeDistributed(mobilenet_transfer,input_shape=(self.frames_to_sample,self.image_height,self.image_width,self.channels)))\n",
    " \n",
    "        \n",
    "        model.add(TimeDistributed(BatchNormalization()))\n",
    "        model.add(TimeDistributed(MaxPooling2D((2, 2))))\n",
    "        model.add(TimeDistributed(Flatten()))\n",
    "\n",
    "        model.add(GRU(gru_cells))\n",
    "        model.add(Dropout(dropout))\n",
    "        \n",
    "        model.add(Dense(dense_neurons,activation='relu'))\n",
    "        model.add(Dropout(dropout))\n",
    "        \n",
    "        model.add(Dense(self.num_classes, activation='softmax'))\n",
    "        \n",
    "        \n",
    "        optimiser = 'adam' #optimizers.Adam()\n",
    "        model.compile(optimizer=optimiser, loss='categorical_crossentropy', metrics=['categorical_accuracy'])\n",
    "        return model\n",
    "    def train_model(self, model, augment_data=False):\n",
    "        train_generator = self.generator(self.train_path, self.train_doc,augment=augment_data)\n",
    "        val_generator = self.generator(self.val_path, self.val_doc)\n",
    "\n",
    "        model_name = 'model_init' + '_' + str(datetime.datetime.now()).replace(' ','').replace(':','_') + '/'\n",
    "    \n",
    "        if not os.path.exists(model_name):\n",
    "            os.mkdir(model_name)\n",
    "        \n",
    "        filepath = model_name + 'model-{epoch:05d}-{loss:.5f}-{categorical_accuracy:.5f}-{val_loss:.5f}-{val_categorical_accuracy:.5f}.h5'\n",
    "\n",
    "        checkpoint = ModelCheckpoint(filepath, monitor='val_loss', verbose=1, save_best_only=False, save_weights_only=False, mode='auto', period=1)\n",
    "        LR = ReduceLROnPlateau(monitor='val_loss', factor=0.2, verbose=1, patience=4)\n",
    "        callbacks_list = [checkpoint, LR]\n",
    "\n",
    "        if (self.num_train_sequences%self.batch_size) == 0:\n",
    "            steps_per_epoch = int(self.num_train_sequences/self.batch_size)\n",
    "        else:\n",
    "            steps_per_epoch = (self.num_train_sequences//self.batch_size) + 1\n",
    "\n",
    "        if (self.num_val_sequences%self.batch_size) == 0:\n",
    "            validation_steps = int(self.num_val_sequences/self.batch_size)\n",
    "        else:\n",
    "            validation_steps = (self.num_val_sequences//self.batch_size) + 1\n",
    "    \n",
    "        history=model.fit_generator(train_generator, steps_per_epoch=steps_per_epoch, epochs=self.num_epochs, verbose=1, \n",
    "                            callbacks=callbacks_list, validation_data=val_generator, \n",
    "                            validation_steps=validation_steps, class_weight=None, workers=1, initial_epoch=0)\n",
    "        return history\n",
    "    def initialize_path(self,project_folder):\n",
    "        self.train_doc = np.random.permutation(open(project_folder + '/' + 'train.csv').readlines())\n",
    "        self.val_doc = np.random.permutation(open(project_folder + '/' + 'val.csv').readlines())\n",
    "        self.train_path = project_folder + '/' + 'train'\n",
    "        self.val_path =  project_folder + '/' + 'val'\n",
    "        self.num_train_sequences = len(self.train_doc)\n",
    "        self.num_val_sequences = len(self.val_doc)\n",
    "        \n",
    "    def initialize_image_properties(self,image_height=100,image_width=100):\n",
    "        self.image_height=image_height\n",
    "        self.image_width=image_width\n",
    "        self.channels=3\n",
    "        self.num_classes=5\n",
    "        self.total_frames=30\n",
    "          \n",
    "    def initialize_hyperparams(self,frames_to_sample=30,batch_size=20,num_epochs=20):\n",
    "        self.frames_to_sample=frames_to_sample\n",
    "        self.batch_size=batch_size\n",
    "        self.num_epochs=num_epochs\n",
    "        \n",
    "        \n",
    "    def generator(self,source_path, folder_list, augment=False):\n",
    "        img_idx = np.round(np.linspace(0,self.total_frames-1,self.frames_to_sample)).astype(int)\n",
    "        batch_size=self.batch_size\n",
    "        while True:\n",
    "            t = np.random.permutation(folder_list)\n",
    "            num_batches = len(t)//batch_size\n",
    "        \n",
    "            for batch in range(num_batches): \n",
    "                batch_data, batch_labels= self.one_batch_data(source_path,t,batch,batch_size,img_idx,augment)\n",
    "                yield batch_data, batch_labels \n",
    "\n",
    "            remaining_seq=len(t)%batch_size\n",
    "        \n",
    "            if (remaining_seq != 0):\n",
    "                batch_data, batch_labels= self.one_batch_data(source_path,t,num_batches,batch_size,img_idx,augment,remaining_seq)\n",
    "                yield batch_data, batch_labels \n",
    "    \n",
    "    \n",
    "    def one_batch_data(self,source_path,t,batch,batch_size,img_idx,augment,remaining_seq=0):\n",
    "    \n",
    "        seq_len = remaining_seq if remaining_seq else batch_size\n",
    "    \n",
    "        batch_data = np.zeros((seq_len,len(img_idx),self.image_height,self.image_width,self.channels)) \n",
    "        batch_labels = np.zeros((seq_len,self.num_classes)) \n",
    "    \n",
    "        if (augment): batch_data_aug = np.zeros((seq_len,len(img_idx),self.image_height,self.image_width,self.channels))\n",
    "        for folder in range(seq_len): \n",
    "            imgs = os.listdir(source_path+'/'+ t[folder + (batch*batch_size)].split(';')[0]) \n",
    "            for idx,item in enumerate(img_idx): \n",
    "                image = imread(source_path+'/'+ t[folder + (batch*batch_size)].strip().split(';')[0]+'/'+imgs[item]).astype(np.float32)\n",
    "                image_resized=imresize(image,(self.image_height,self.image_width,3))\n",
    "            \n",
    "\n",
    "                batch_data[folder,idx,:,:,0] = (image_resized[:,:,0])/255\n",
    "                batch_data[folder,idx,:,:,1] = (image_resized[:,:,1])/255\n",
    "                batch_data[folder,idx,:,:,2] = (image_resized[:,:,2])/255\n",
    "            \n",
    "                if (augment):\n",
    "                    shifted = cv2.warpAffine(image, \n",
    "                                             np.float32([[1, 0, np.random.randint(-30,30)],[0, 1, np.random.randint(-30,30)]]), \n",
    "                                            (image.shape[1], image.shape[0]))\n",
    "                    \n",
    "                    gray = cv2.cvtColor(shifted,cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "                    x0, y0 = np.argwhere(gray > 0).min(axis=0)\n",
    "                    x1, y1 = np.argwhere(gray > 0).max(axis=0) \n",
    "                    \n",
    "                    cropped=shifted[x0:x1,y0:y1,:]\n",
    "                    \n",
    "                    image_resized=imresize(cropped,(self.image_height,self.image_width,3))\n",
    "                    \n",
    "                    #shifted = cv2.warpAffine(image_resized, \n",
    "                    #                        np.float32([[1, 0, np.random.randint(-3,3)],[0, 1, np.random.randint(-3,3)]]), \n",
    "                    #                        (image_resized.shape[1], image_resized.shape[0]))\n",
    "            \n",
    "                    batch_data_aug[folder,idx,:,:,0] = (image_resized[:,:,0])/255\n",
    "                    batch_data_aug[folder,idx,:,:,1] = (image_resized[:,:,1])/255\n",
    "                    batch_data_aug[folder,idx,:,:,2] = (image_resized[:,:,2])/255\n",
    "                \n",
    "            \n",
    "            batch_labels[folder, int(t[folder + (batch*batch_size)].strip().split(';')[2])] = 1\n",
    "            \n",
    "    \n",
    "        if (augment):\n",
    "            batch_data=np.concatenate([batch_data,batch_data_aug])\n",
    "            batch_labels=np.concatenate([batch_labels,batch_labels])\n",
    "\n",
    "        \n",
    "        return(batch_data,batch_labels)\n",
    "\n",
    "    def train_model(self, model, augment_data=False):\n",
    "        train_generator = self.generator(self.train_path, self.train_doc,augment=augment_data)\n",
    "        val_generator = self.generator(self.val_path, self.val_doc)\n",
    "\n",
    "        model_name = 'model_init' + '_' + str(datetime.datetime.now()).replace(' ','').replace(':','_') + '/'\n",
    "    \n",
    "        if not os.path.exists(model_name):\n",
    "            os.mkdir(model_name)\n",
    "        \n",
    "        filepath = model_name + 'model-{epoch:05d}-{loss:.5f}-{categorical_accuracy:.5f}-{val_loss:.5f}-{val_categorical_accuracy:.5f}.h5'\n",
    "\n",
    "        checkpoint = ModelCheckpoint(filepath, monitor='val_loss', verbose=1, save_best_only=False, save_weights_only=False, mode='auto', period=1)\n",
    "        LR = ReduceLROnPlateau(monitor='val_loss', factor=0.2, verbose=1, patience=4)\n",
    "        callbacks_list = [checkpoint, LR]\n",
    "\n",
    "        if (self.num_train_sequences%self.batch_size) == 0:\n",
    "            steps_per_epoch = int(self.num_train_sequences/self.batch_size)\n",
    "        else:\n",
    "            steps_per_epoch = (self.num_train_sequences//self.batch_size) + 1\n",
    "\n",
    "        if (self.num_val_sequences%self.batch_size) == 0:\n",
    "            validation_steps = int(self.num_val_sequences/self.batch_size)\n",
    "        else:\n",
    "            validation_steps = (self.num_val_sequences//self.batch_size) + 1\n",
    "    \n",
    "        history=model.fit_generator(train_generator, steps_per_epoch=steps_per_epoch, epochs=self.num_epochs, verbose=1, \n",
    "                            callbacks=callbacks_list, validation_data=val_generator, \n",
    "                            validation_steps=validation_steps, class_weight=None, workers=1, initial_epoch=0)\n",
    "        return history\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_4\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " time_distributed_5 (TimeDis  (None, 16, 3, 3, 1024)   3228864   \n",
      " tributed)                                                       \n",
      "                                                                 \n",
      " time_distributed_6 (TimeDis  (None, 16, 3, 3, 1024)   4096      \n",
      " tributed)                                                       \n",
      "                                                                 \n",
      " time_distributed_7 (TimeDis  (None, 16, 1, 1, 1024)   0         \n",
      " tributed)                                                       \n",
      "                                                                 \n",
      " time_distributed_8 (TimeDis  (None, 16, 1024)         0         \n",
      " tributed)                                                       \n",
      "                                                                 \n",
      " gru_2 (GRU)                 (None, 128)               443136    \n",
      "                                                                 \n",
      " dropout_7 (Dropout)         (None, 128)               0         \n",
      "                                                                 \n",
      " dense_10 (Dense)            (None, 128)               16512     \n",
      "                                                                 \n",
      " dropout_8 (Dropout)         (None, 128)               0         \n",
      "                                                                 \n",
      " dense_11 (Dense)            (None, 5)                 645       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 3,693,253\n",
      "Trainable params: 3,669,317\n",
      "Non-trainable params: 23,936\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "rnn_cnn_tl2=RNNCNN_TL2()\n",
    "rnn_cnn_tl2.initialize_path(project_folder)\n",
    "rnn_cnn_tl2.initialize_image_properties(image_height=120,image_width=120)\n",
    "rnn_cnn_tl2.initialize_hyperparams(frames_to_sample=16,batch_size=5,num_epochs=20)\n",
    "rnn_cnn_tl2_model=rnn_cnn_tl2.define_model(gru_cells=128,dense_neurons=128,dropout=0.25)\n",
    "rnn_cnn_tl2_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Params: 3693253\n",
      "WARNING:tensorflow:`period` argument is deprecated. Please use `save_freq` to specify the frequency in number of batches seen.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_375/971982952.py:174: UserWarning: `Model.fit_generator` is deprecated and will be removed in a future version. Please use `Model.fit`, which supports generators.\n",
      "  history=model.fit_generator(train_generator, steps_per_epoch=steps_per_epoch, epochs=self.num_epochs, verbose=1,\n",
      "/tmp/ipykernel_375/971982952.py:108: DeprecationWarning:     `imread` is deprecated!\n",
      "    `imread` is deprecated in SciPy 1.0.0, and will be removed in 1.2.0.\n",
      "    Use ``imageio.imread`` instead.\n",
      "  image = imread(source_path+'/'+ t[folder + (batch*batch_size)].strip().split(';')[0]+'/'+imgs[item]).astype(np.float32)\n",
      "/tmp/ipykernel_375/971982952.py:109: DeprecationWarning:     `imresize` is deprecated!\n",
      "    `imresize` is deprecated in SciPy 1.0.0, and will be removed in 1.2.0.\n",
      "    Use ``skimage.transform.resize`` instead.\n",
      "  image_resized=imresize(image,(self.image_height,self.image_width,3))\n",
      "/tmp/ipykernel_375/971982952.py:128: DeprecationWarning:     `imresize` is deprecated!\n",
      "    `imresize` is deprecated in SciPy 1.0.0, and will be removed in 1.2.0.\n",
      "    Use ``skimage.transform.resize`` instead.\n",
      "  image_resized=imresize(cropped,(self.image_height,self.image_width,3))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "133/133 [==============================] - ETA: 0s - loss: 0.8887 - categorical_accuracy: 0.6493\n",
      "Epoch 00001: saving model to model_init_2022-05-1511_46_47.948988/model-00001-0.88868-0.64932-0.28528-0.91000.h5\n",
      "133/133 [==============================] - 94s 682ms/step - loss: 0.8887 - categorical_accuracy: 0.6493 - val_loss: 0.2853 - val_categorical_accuracy: 0.9100 - lr: 0.0010\n",
      "Epoch 2/20\n",
      "133/133 [==============================] - ETA: 0s - loss: 0.3335 - categorical_accuracy: 0.8959\n",
      "Epoch 00002: saving model to model_init_2022-05-1511_46_47.948988/model-00002-0.33353-0.89593-0.34851-0.87000.h5\n",
      "133/133 [==============================] - 90s 677ms/step - loss: 0.3335 - categorical_accuracy: 0.8959 - val_loss: 0.3485 - val_categorical_accuracy: 0.8700 - lr: 0.0010\n",
      "Epoch 3/20\n",
      "133/133 [==============================] - ETA: 0s - loss: 0.2506 - categorical_accuracy: 0.9201\n",
      "Epoch 00003: saving model to model_init_2022-05-1511_46_47.948988/model-00003-0.25055-0.92006-0.33366-0.86000.h5\n",
      "133/133 [==============================] - 90s 682ms/step - loss: 0.2506 - categorical_accuracy: 0.9201 - val_loss: 0.3337 - val_categorical_accuracy: 0.8600 - lr: 0.0010\n",
      "Epoch 4/20\n",
      "133/133 [==============================] - ETA: 0s - loss: 0.1721 - categorical_accuracy: 0.9472\n",
      "Epoch 00004: saving model to model_init_2022-05-1511_46_47.948988/model-00004-0.17206-0.94721-0.18623-0.93000.h5\n",
      "133/133 [==============================] - 89s 674ms/step - loss: 0.1721 - categorical_accuracy: 0.9472 - val_loss: 0.1862 - val_categorical_accuracy: 0.9300 - lr: 0.0010\n",
      "Epoch 5/20\n",
      "133/133 [==============================] - ETA: 0s - loss: 0.2513 - categorical_accuracy: 0.9268\n",
      "Epoch 00005: saving model to model_init_2022-05-1511_46_47.948988/model-00005-0.25130-0.92685-0.11380-0.94000.h5\n",
      "133/133 [==============================] - 90s 680ms/step - loss: 0.2513 - categorical_accuracy: 0.9268 - val_loss: 0.1138 - val_categorical_accuracy: 0.9400 - lr: 0.0010\n",
      "Epoch 6/20\n",
      "133/133 [==============================] - ETA: 0s - loss: 0.1698 - categorical_accuracy: 0.9449\n",
      "Epoch 00006: saving model to model_init_2022-05-1511_46_47.948988/model-00006-0.16977-0.94495-0.02744-1.00000.h5\n",
      "133/133 [==============================] - 91s 686ms/step - loss: 0.1698 - categorical_accuracy: 0.9449 - val_loss: 0.0274 - val_categorical_accuracy: 1.0000 - lr: 0.0010\n",
      "Epoch 7/20\n",
      "133/133 [==============================] - ETA: 0s - loss: 0.0604 - categorical_accuracy: 0.9849\n",
      "Epoch 00007: saving model to model_init_2022-05-1511_46_47.948988/model-00007-0.06038-0.98492-0.02972-0.99000.h5\n",
      "133/133 [==============================] - 91s 686ms/step - loss: 0.0604 - categorical_accuracy: 0.9849 - val_loss: 0.0297 - val_categorical_accuracy: 0.9900 - lr: 0.0010\n",
      "Epoch 8/20\n",
      "133/133 [==============================] - ETA: 0s - loss: 0.0866 - categorical_accuracy: 0.9751\n",
      "Epoch 00008: saving model to model_init_2022-05-1511_46_47.948988/model-00008-0.08664-0.97511-0.14459-0.94000.h5\n",
      "133/133 [==============================] - 88s 662ms/step - loss: 0.0866 - categorical_accuracy: 0.9751 - val_loss: 0.1446 - val_categorical_accuracy: 0.9400 - lr: 0.0010\n",
      "Epoch 9/20\n",
      "133/133 [==============================] - ETA: 0s - loss: 0.1617 - categorical_accuracy: 0.9502\n",
      "Epoch 00009: saving model to model_init_2022-05-1511_46_47.948988/model-00009-0.16173-0.95023-0.15721-0.93000.h5\n",
      "133/133 [==============================] - 88s 665ms/step - loss: 0.1617 - categorical_accuracy: 0.9502 - val_loss: 0.1572 - val_categorical_accuracy: 0.9300 - lr: 0.0010\n",
      "Epoch 10/20\n",
      "133/133 [==============================] - ETA: 0s - loss: 0.2005 - categorical_accuracy: 0.9397\n",
      "Epoch 00010: saving model to model_init_2022-05-1511_46_47.948988/model-00010-0.20049-0.93967-0.13215-0.94000.h5\n",
      "\n",
      "Epoch 00010: ReduceLROnPlateau reducing learning rate to 0.00020000000949949026.\n",
      "133/133 [==============================] - 88s 663ms/step - loss: 0.2005 - categorical_accuracy: 0.9397 - val_loss: 0.1321 - val_categorical_accuracy: 0.9400 - lr: 0.0010\n",
      "Epoch 11/20\n",
      "133/133 [==============================] - ETA: 0s - loss: 0.1040 - categorical_accuracy: 0.9638\n",
      "Epoch 00011: saving model to model_init_2022-05-1511_46_47.948988/model-00011-0.10404-0.96380-0.11622-0.98000.h5\n",
      "133/133 [==============================] - 87s 654ms/step - loss: 0.1040 - categorical_accuracy: 0.9638 - val_loss: 0.1162 - val_categorical_accuracy: 0.9800 - lr: 2.0000e-04\n",
      "Epoch 12/20\n",
      "133/133 [==============================] - ETA: 0s - loss: 0.0513 - categorical_accuracy: 0.9879\n",
      "Epoch 00012: saving model to model_init_2022-05-1511_46_47.948988/model-00012-0.05127-0.98793-0.04566-0.99000.h5\n",
      "133/133 [==============================] - 87s 658ms/step - loss: 0.0513 - categorical_accuracy: 0.9879 - val_loss: 0.0457 - val_categorical_accuracy: 0.9900 - lr: 2.0000e-04\n",
      "Epoch 13/20\n",
      "133/133 [==============================] - ETA: 0s - loss: 0.0422 - categorical_accuracy: 0.9894\n",
      "Epoch 00013: saving model to model_init_2022-05-1511_46_47.948988/model-00013-0.04223-0.98944-0.03807-0.99000.h5\n",
      "133/133 [==============================] - 86s 652ms/step - loss: 0.0422 - categorical_accuracy: 0.9894 - val_loss: 0.0381 - val_categorical_accuracy: 0.9900 - lr: 2.0000e-04\n",
      "Epoch 14/20\n",
      "133/133 [==============================] - ETA: 0s - loss: 0.0296 - categorical_accuracy: 0.9917\n",
      "Epoch 00014: saving model to model_init_2022-05-1511_46_47.948988/model-00014-0.02958-0.99170-0.05012-0.99000.h5\n",
      "\n",
      "Epoch 00014: ReduceLROnPlateau reducing learning rate to 4.0000001899898055e-05.\n",
      "133/133 [==============================] - 86s 653ms/step - loss: 0.0296 - categorical_accuracy: 0.9917 - val_loss: 0.0501 - val_categorical_accuracy: 0.9900 - lr: 2.0000e-04\n",
      "Epoch 15/20\n",
      "133/133 [==============================] - ETA: 0s - loss: 0.0235 - categorical_accuracy: 0.9947\n",
      "Epoch 00015: saving model to model_init_2022-05-1511_46_47.948988/model-00015-0.02349-0.99472-0.05512-0.99000.h5\n",
      "133/133 [==============================] - 88s 666ms/step - loss: 0.0235 - categorical_accuracy: 0.9947 - val_loss: 0.0551 - val_categorical_accuracy: 0.9900 - lr: 4.0000e-05\n",
      "Epoch 16/20\n",
      "133/133 [==============================] - ETA: 0s - loss: 0.0286 - categorical_accuracy: 0.9925\n",
      "Epoch 00016: saving model to model_init_2022-05-1511_46_47.948988/model-00016-0.02858-0.99246-0.05179-0.99000.h5\n",
      "133/133 [==============================] - 87s 656ms/step - loss: 0.0286 - categorical_accuracy: 0.9925 - val_loss: 0.0518 - val_categorical_accuracy: 0.9900 - lr: 4.0000e-05\n",
      "Epoch 17/20\n",
      "133/133 [==============================] - ETA: 0s - loss: 0.0133 - categorical_accuracy: 0.9985\n",
      "Epoch 00017: saving model to model_init_2022-05-1511_46_47.948988/model-00017-0.01332-0.99849-0.04085-0.99000.h5\n",
      "133/133 [==============================] - 87s 657ms/step - loss: 0.0133 - categorical_accuracy: 0.9985 - val_loss: 0.0408 - val_categorical_accuracy: 0.9900 - lr: 4.0000e-05\n",
      "Epoch 18/20\n",
      "133/133 [==============================] - ETA: 0s - loss: 0.0136 - categorical_accuracy: 0.9977\n",
      "Epoch 00018: saving model to model_init_2022-05-1511_46_47.948988/model-00018-0.01362-0.99774-0.01264-1.00000.h5\n",
      "133/133 [==============================] - 89s 673ms/step - loss: 0.0136 - categorical_accuracy: 0.9977 - val_loss: 0.0126 - val_categorical_accuracy: 1.0000 - lr: 4.0000e-05\n",
      "Epoch 19/20\n",
      "133/133 [==============================] - ETA: 0s - loss: 0.0169 - categorical_accuracy: 0.9970\n",
      "Epoch 00019: saving model to model_init_2022-05-1511_46_47.948988/model-00019-0.01690-0.99698-0.02200-1.00000.h5\n",
      "133/133 [==============================] - 89s 669ms/step - loss: 0.0169 - categorical_accuracy: 0.9970 - val_loss: 0.0220 - val_categorical_accuracy: 1.0000 - lr: 4.0000e-05\n",
      "Epoch 20/20\n",
      "133/133 [==============================] - ETA: 0s - loss: 0.0118 - categorical_accuracy: 0.9977\n",
      "Epoch 00020: saving model to model_init_2022-05-1511_46_47.948988/model-00020-0.01175-0.99774-0.03558-0.99000.h5\n",
      "133/133 [==============================] - 88s 669ms/step - loss: 0.0118 - categorical_accuracy: 0.9977 - val_loss: 0.0356 - val_categorical_accuracy: 0.9900 - lr: 4.0000e-05\n"
     ]
    }
   ],
   "source": [
    "print(\"Total Params:\", rnn_cnn_tl2_model.count_params())\n",
    "history_model19=rnn_cnn_tl2.train_model(rnn_cnn_tl2_model,augment_data=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Looks like overfitting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model 6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### mobilenet 2d+gru(batch size 20, frames to sample 15),epoch 25 optimizer ADAM and training all weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 40 #experiment with the batch size\n",
    "x = 30\n",
    "y = 120\n",
    "z = 120\n",
    "\n",
    "def generator(source_path, folder_list, batch_size):\n",
    "    print( 'Source path = ', source_path, '; batch size =', batch_size)\n",
    "    img_idx = [x for x in range(0,x)] #create a list of image numbers you want to use for a particular video\n",
    "    while True:\n",
    "        t = np.random.permutation(folder_list)\n",
    "        num_batches = len(folder_list)//batch_size # calculate the number of batches\n",
    "        for batch in range(num_batches): # we iterate over the number of batches\n",
    "            batch_data = np.zeros((batch_size,x,y,z,3)) # x is the number of images you use for each video, (y,z) is the final size of the input images and 3 is the number of channels RGB\n",
    "            batch_labels = np.zeros((batch_size,5)) # batch_labels is the one hot representation of the output\n",
    "            for folder in range(batch_size): # iterate over the batch_size\n",
    "                imgs = os.listdir(source_path+'/'+ t[folder + (batch*batch_size)].split(';')[0]) # read all the images in the folder\n",
    "                for idx,item in enumerate(img_idx): #  Iterate iver the frames/images of a folder to read them in\n",
    "                    image = imread(source_path+'/'+ t[folder + (batch*batch_size)].strip().split(';')[0]+'/'+imgs[item]).astype(np.float32)\n",
    "                    \n",
    "                    #crop the images and resize them. Note that the images are of 2 different shape \n",
    "                    \n",
    "                    resized_image = imresize(image,(y,z))\n",
    "                    resized_image = resized_image/255\n",
    "                         \n",
    "                    #and the conv3D will throw error if the inputs in a batch have different shapes\n",
    "                    \n",
    "                    \n",
    "                    \n",
    "                    batch_data[folder,idx,:,:,0] = (resized_image[:,:,0]) #normalise and feed in the image\n",
    "                    batch_data[folder,idx,:,:,1] = (resized_image[:,:,1]) #normalise and feed in the image\n",
    "                    batch_data[folder,idx,:,:,2] = (resized_image[:,:,2]) #normalise and feed in the image\n",
    "                    \n",
    "                batch_labels[folder, int(t[folder + (batch*batch_size)].strip().split(';')[2])] = 1\n",
    "            yield batch_data, batch_labels #you yield the batch_data and the batch_labels, remember what does yield do\n",
    "\n",
    "        \n",
    "        # write the code for the remaining data points which are left after full batches\n",
    "        if(len(folder_list) != batch_size*num_batches):\n",
    "            batch_size = len(folder_list) - (batch_size*num_batches)\n",
    "            batch_data = np.zeros((batch_size,x,y,z,3))\n",
    "            batch_labels = np.zeros((batch_size,5))\n",
    "            for folder in range(batch_size):\n",
    "                imgs = os.listdir(source_path+'/'+ t[folder + (batch*batch_size)].split(';')[0]) # read all the images in the folder\n",
    "                for idx,item in enumerate(img_idx): #  Iterate iver the frames/images of a folder to read them in\n",
    "                    image = imread(source_path+'/'+ t[folder + (batch*batch_size)].strip().split(';')[0]+'/'+imgs[item]).astype(np.float32)\n",
    "                    \n",
    "                    #crop the images and resize them. Note that the images are of 2 different shape \n",
    "                    #and the conv3D will throw error if the inputs in a batch have different shapes\n",
    "                    resized_image = imresize(image,(y,z))\n",
    "                    resized_image = resized_image/255\n",
    "                    \n",
    "                    batch_data[folder,idx,:,:,0] = (resized_image[:,:,0])#normalise and feed in the image\n",
    "                    batch_data[folder,idx,:,:,1] = (resized_image[:,:,1])#normalise and feed in the image\n",
    "                    batch_data[folder,idx,:,:,2] = (resized_image[:,:,2])#normalise and feed in the image\n",
    "                    \n",
    "                batch_labels[folder, int(t[folder + (batch*batch_size)].strip().split(';')[2])] = 1\n",
    "            yield batch_data, batch_labels #you yield the batch_data and the batch_labels, remember what does yield do\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:`input_shape` is undefined or non-square, or `rows` is not in [128, 160, 192, 224]. Weights for input shape (224, 224) will be loaded as the default.\n"
     ]
    }
   ],
   "source": [
    "from keras.applications import mobilenet\n",
    "\n",
    "mobilenet_transfer = mobilenet.MobileNet(weights='imagenet', include_top=False)\n",
    "\n",
    "class RNNCNN_TL2(ModelBuilderMoreAugmentation):\n",
    "    \n",
    "    def define_model(self,gru_cells=64,dense_neurons=64,dropout=0.25):\n",
    "        \n",
    "        model = Sequential()\n",
    "        model.add(TimeDistributed(mobilenet_transfer,input_shape=(self.frames_to_sample,self.image_height,self.image_width,self.channels)))\n",
    " \n",
    "        \n",
    "        model.add(TimeDistributed(BatchNormalization()))\n",
    "        model.add(TimeDistributed(MaxPooling2D((2, 2))))\n",
    "        model.add(TimeDistributed(Flatten()))\n",
    "\n",
    "        model.add(GRU(gru_cells))\n",
    "        model.add(Dropout(dropout))\n",
    "        \n",
    "        model.add(Dense(dense_neurons,activation='relu'))\n",
    "        model.add(Dropout(dropout))\n",
    "        \n",
    "        model.add(Dense(self.num_classes, activation='softmax'))\n",
    "        \n",
    "        optimiser = tf.keras.optimizers.Adam(0.00001)\n",
    "        #optimiser = 'adam' #optimizers.Adam()\n",
    "        model.compile(optimizer=optimiser, loss='categorical_crossentropy', metrics=['categorical_accuracy'])\n",
    "        return model\n",
    "    def train_model(self, model, augment_data=False):\n",
    "        train_generator = self.generator(self.train_path, self.train_doc,augment=augment_data)\n",
    "        val_generator = self.generator(self.val_path, self.val_doc)\n",
    "\n",
    "        model_name = 'model_init' + '_' + str(datetime.datetime.now()).replace(' ','').replace(':','_') + '/'\n",
    "    \n",
    "        if not os.path.exists(model_name):\n",
    "            os.mkdir(model_name)\n",
    "        \n",
    "        filepath = model_name + 'model-{epoch:05d}-{loss:.5f}-{categorical_accuracy:.5f}-{val_loss:.5f}-{val_categorical_accuracy:.5f}.h5'\n",
    "\n",
    "        checkpoint = ModelCheckpoint(filepath, monitor='val_loss', verbose=1, save_best_only=False, save_weights_only=False, mode='auto', period=1)\n",
    "        LR = ReduceLROnPlateau(monitor='val_loss', factor=0.2, verbose=1, patience=4)\n",
    "        callbacks_list = [checkpoint, LR]\n",
    "\n",
    "        if (self.num_train_sequences%self.batch_size) == 0:\n",
    "            steps_per_epoch = int(self.num_train_sequences/self.batch_size)\n",
    "        else:\n",
    "            steps_per_epoch = (self.num_train_sequences//self.batch_size) + 1\n",
    "\n",
    "        if (self.num_val_sequences%self.batch_size) == 0:\n",
    "            validation_steps = int(self.num_val_sequences/self.batch_size)\n",
    "        else:\n",
    "            validation_steps = (self.num_val_sequences//self.batch_size) + 1\n",
    "    \n",
    "        history=model.fit_generator(train_generator, steps_per_epoch=steps_per_epoch, epochs=self.num_epochs, verbose=1, \n",
    "                            callbacks=callbacks_list, validation_data=val_generator, \n",
    "                            validation_steps=validation_steps, class_weight=None, workers=1, initial_epoch=0)\n",
    "        return history\n",
    "    def initialize_path(self,project_folder):\n",
    "        self.train_doc = np.random.permutation(open(project_folder + '/' + 'train.csv').readlines())\n",
    "        self.val_doc = np.random.permutation(open(project_folder + '/' + 'val.csv').readlines())\n",
    "        self.train_path = project_folder + '/' + 'train'\n",
    "        self.val_path =  project_folder + '/' + 'val'\n",
    "        self.num_train_sequences = len(self.train_doc)\n",
    "        self.num_val_sequences = len(self.val_doc)\n",
    "        \n",
    "    def initialize_image_properties(self,image_height=100,image_width=100):\n",
    "        self.image_height=image_height\n",
    "        self.image_width=image_width\n",
    "        self.channels=3\n",
    "        self.num_classes=5\n",
    "        self.total_frames=30\n",
    "          \n",
    "    def initialize_hyperparams(self,frames_to_sample=30,batch_size=20,num_epochs=20):\n",
    "        self.frames_to_sample=frames_to_sample\n",
    "        self.batch_size=batch_size\n",
    "        self.num_epochs=num_epochs\n",
    "        \n",
    "        \n",
    "    def generator(self,source_path, folder_list, augment=False):\n",
    "        img_idx = np.round(np.linspace(0,self.total_frames-1,self.frames_to_sample)).astype(int)\n",
    "        batch_size=self.batch_size\n",
    "        while True:\n",
    "            t = np.random.permutation(folder_list)\n",
    "            num_batches = len(t)//batch_size\n",
    "        \n",
    "            for batch in range(num_batches): \n",
    "                batch_data, batch_labels= self.one_batch_data(source_path,t,batch,batch_size,img_idx,augment)\n",
    "                yield batch_data, batch_labels \n",
    "\n",
    "            remaining_seq=len(t)%batch_size\n",
    "        \n",
    "            if (remaining_seq != 0):\n",
    "                batch_data, batch_labels= self.one_batch_data(source_path,t,num_batches,batch_size,img_idx,augment,remaining_seq)\n",
    "                yield batch_data, batch_labels \n",
    "    \n",
    "    \n",
    "    def one_batch_data(self,source_path,t,batch,batch_size,img_idx,augment,remaining_seq=0):\n",
    "    \n",
    "        seq_len = remaining_seq if remaining_seq else batch_size\n",
    "    \n",
    "        batch_data = np.zeros((seq_len,len(img_idx),self.image_height,self.image_width,self.channels)) \n",
    "        batch_labels = np.zeros((seq_len,self.num_classes)) \n",
    "    \n",
    "        if (augment): batch_data_aug = np.zeros((seq_len,len(img_idx),self.image_height,self.image_width,self.channels))\n",
    "        for folder in range(seq_len): \n",
    "            imgs = os.listdir(source_path+'/'+ t[folder + (batch*batch_size)].split(';')[0]) \n",
    "            for idx,item in enumerate(img_idx): \n",
    "                image = imread(source_path+'/'+ t[folder + (batch*batch_size)].strip().split(';')[0]+'/'+imgs[item]).astype(np.float32)\n",
    "                image_resized=imresize(image,(self.image_height,self.image_width,3))\n",
    "            \n",
    "\n",
    "                batch_data[folder,idx,:,:,0] = (image_resized[:,:,0])/255\n",
    "                batch_data[folder,idx,:,:,1] = (image_resized[:,:,1])/255\n",
    "                batch_data[folder,idx,:,:,2] = (image_resized[:,:,2])/255\n",
    "            \n",
    "                if (augment):\n",
    "                    shifted = cv2.warpAffine(image, \n",
    "                                             np.float32([[1, 0, np.random.randint(-30,30)],[0, 1, np.random.randint(-30,30)]]), \n",
    "                                            (image.shape[1], image.shape[0]))\n",
    "                    \n",
    "                    gray = cv2.cvtColor(shifted,cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "                    x0, y0 = np.argwhere(gray > 0).min(axis=0)\n",
    "                    x1, y1 = np.argwhere(gray > 0).max(axis=0) \n",
    "                    \n",
    "                    cropped=shifted[x0:x1,y0:y1,:]\n",
    "                    \n",
    "                    image_resized=imresize(cropped,(self.image_height,self.image_width,3))\n",
    "                    \n",
    "                    #shifted = cv2.warpAffine(image_resized, \n",
    "                    #                        np.float32([[1, 0, np.random.randint(-3,3)],[0, 1, np.random.randint(-3,3)]]), \n",
    "                    #                        (image_resized.shape[1], image_resized.shape[0]))\n",
    "            \n",
    "                    batch_data_aug[folder,idx,:,:,0] = (image_resized[:,:,0])/255\n",
    "                    batch_data_aug[folder,idx,:,:,1] = (image_resized[:,:,1])/255\n",
    "                    batch_data_aug[folder,idx,:,:,2] = (image_resized[:,:,2])/255\n",
    "                \n",
    "            \n",
    "            batch_labels[folder, int(t[folder + (batch*batch_size)].strip().split(';')[2])] = 1\n",
    "            \n",
    "    \n",
    "        if (augment):\n",
    "            batch_data=np.concatenate([batch_data,batch_data_aug])\n",
    "            batch_labels=np.concatenate([batch_labels,batch_labels])\n",
    "\n",
    "        \n",
    "        return(batch_data,batch_labels)\n",
    "\n",
    "    def train_model(self, model, augment_data=False):\n",
    "        train_generator = self.generator(self.train_path, self.train_doc,augment=augment_data)\n",
    "        val_generator = self.generator(self.val_path, self.val_doc)\n",
    "\n",
    "        model_name = 'model_init' + '_' + str(datetime.datetime.now()).replace(' ','').replace(':','_') + '/'\n",
    "    \n",
    "        if not os.path.exists(model_name):\n",
    "            os.mkdir(model_name)\n",
    "        \n",
    "        filepath = model_name + 'model-{epoch:05d}-{loss:.5f}-{categorical_accuracy:.5f}-{val_loss:.5f}-{val_categorical_accuracy:.5f}.h5'\n",
    "\n",
    "        checkpoint = ModelCheckpoint(filepath, monitor='val_loss', verbose=1, save_best_only=False, save_weights_only=False, mode='auto', period=1)\n",
    "        LR = ReduceLROnPlateau(monitor='val_loss', factor=0.2, verbose=1, patience=4)\n",
    "        callbacks_list = [checkpoint, LR]\n",
    "\n",
    "        if (self.num_train_sequences%self.batch_size) == 0:\n",
    "            steps_per_epoch = int(self.num_train_sequences/self.batch_size)\n",
    "        else:\n",
    "            steps_per_epoch = (self.num_train_sequences//self.batch_size) + 1\n",
    "\n",
    "        if (self.num_val_sequences%self.batch_size) == 0:\n",
    "            validation_steps = int(self.num_val_sequences/self.batch_size)\n",
    "        else:\n",
    "            validation_steps = (self.num_val_sequences//self.batch_size) + 1\n",
    "    \n",
    "        history=model.fit_generator(train_generator, steps_per_epoch=steps_per_epoch, epochs=self.num_epochs, verbose=1, \n",
    "                            callbacks=callbacks_list, validation_data=val_generator, \n",
    "                            validation_steps=validation_steps, class_weight=None, workers=1, initial_epoch=0)\n",
    "        return history\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_5\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " time_distributed_9 (TimeDis  (None, 15, 3, 3, 1024)   3228864   \n",
      " tributed)                                                       \n",
      "                                                                 \n",
      " time_distributed_10 (TimeDi  (None, 15, 3, 3, 1024)   4096      \n",
      " stributed)                                                      \n",
      "                                                                 \n",
      " time_distributed_11 (TimeDi  (None, 15, 1, 1, 1024)   0         \n",
      " stributed)                                                      \n",
      "                                                                 \n",
      " time_distributed_12 (TimeDi  (None, 15, 1024)         0         \n",
      " stributed)                                                      \n",
      "                                                                 \n",
      " gru_3 (GRU)                 (None, 128)               443136    \n",
      "                                                                 \n",
      " dropout_9 (Dropout)         (None, 128)               0         \n",
      "                                                                 \n",
      " dense_12 (Dense)            (None, 128)               16512     \n",
      "                                                                 \n",
      " dropout_10 (Dropout)        (None, 128)               0         \n",
      "                                                                 \n",
      " dense_13 (Dense)            (None, 5)                 645       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 3,693,253\n",
      "Trainable params: 3,669,317\n",
      "Non-trainable params: 23,936\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "rnn_cnn_tl2=RNNCNN_TL2()\n",
    "rnn_cnn_tl2.initialize_path(project_folder)\n",
    "rnn_cnn_tl2.initialize_image_properties(image_height=120,image_width=120)\n",
    "rnn_cnn_tl2.initialize_hyperparams(frames_to_sample=15,batch_size=20,num_epochs=25)\n",
    "rnn_cnn_tl2_model=rnn_cnn_tl2.define_model(gru_cells=128,dense_neurons=128,dropout=0.15)\n",
    "rnn_cnn_tl2_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Params: 3693253\n",
      "WARNING:tensorflow:`period` argument is deprecated. Please use `save_freq` to specify the frequency in number of batches seen.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_375/1420973225.py:174: UserWarning: `Model.fit_generator` is deprecated and will be removed in a future version. Please use `Model.fit`, which supports generators.\n",
      "  history=model.fit_generator(train_generator, steps_per_epoch=steps_per_epoch, epochs=self.num_epochs, verbose=1,\n",
      "/tmp/ipykernel_375/1420973225.py:108: DeprecationWarning:     `imread` is deprecated!\n",
      "    `imread` is deprecated in SciPy 1.0.0, and will be removed in 1.2.0.\n",
      "    Use ``imageio.imread`` instead.\n",
      "  image = imread(source_path+'/'+ t[folder + (batch*batch_size)].strip().split(';')[0]+'/'+imgs[item]).astype(np.float32)\n",
      "/tmp/ipykernel_375/1420973225.py:109: DeprecationWarning:     `imresize` is deprecated!\n",
      "    `imresize` is deprecated in SciPy 1.0.0, and will be removed in 1.2.0.\n",
      "    Use ``skimage.transform.resize`` instead.\n",
      "  image_resized=imresize(image,(self.image_height,self.image_width,3))\n",
      "/tmp/ipykernel_375/1420973225.py:128: DeprecationWarning:     `imresize` is deprecated!\n",
      "    `imresize` is deprecated in SciPy 1.0.0, and will be removed in 1.2.0.\n",
      "    Use ``skimage.transform.resize`` instead.\n",
      "  image_resized=imresize(cropped,(self.image_height,self.image_width,3))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/25\n",
      "34/34 [==============================] - ETA: 0s - loss: 1.7230 - categorical_accuracy: 0.2323\n",
      "Epoch 00001: saving model to model_init_2022-05-1512_21_22.342108/model-00001-1.72297-0.23228-1.75675-0.26000.h5\n",
      "34/34 [==============================] - 92s 3s/step - loss: 1.7230 - categorical_accuracy: 0.2323 - val_loss: 1.7568 - val_categorical_accuracy: 0.2600 - lr: 1.0000e-05\n",
      "Epoch 2/25\n",
      "34/34 [==============================] - ETA: 0s - loss: 1.5896 - categorical_accuracy: 0.2798\n",
      "Epoch 00002: saving model to model_init_2022-05-1512_21_22.342108/model-00002-1.58965-0.27979-1.61517-0.29000.h5\n",
      "34/34 [==============================] - 84s 3s/step - loss: 1.5896 - categorical_accuracy: 0.2798 - val_loss: 1.6152 - val_categorical_accuracy: 0.2900 - lr: 1.0000e-05\n",
      "Epoch 3/25\n",
      "34/34 [==============================] - ETA: 0s - loss: 1.5006 - categorical_accuracy: 0.3484\n",
      "Epoch 00003: saving model to model_init_2022-05-1512_21_22.342108/model-00003-1.50060-0.34842-1.57798-0.30000.h5\n",
      "34/34 [==============================] - 86s 3s/step - loss: 1.5006 - categorical_accuracy: 0.3484 - val_loss: 1.5780 - val_categorical_accuracy: 0.3000 - lr: 1.0000e-05\n",
      "Epoch 4/25\n",
      "34/34 [==============================] - ETA: 0s - loss: 1.4025 - categorical_accuracy: 0.4261\n",
      "Epoch 00004: saving model to model_init_2022-05-1512_21_22.342108/model-00004-1.40254-0.42609-1.45313-0.38000.h5\n",
      "34/34 [==============================] - 85s 3s/step - loss: 1.4025 - categorical_accuracy: 0.4261 - val_loss: 1.4531 - val_categorical_accuracy: 0.3800 - lr: 1.0000e-05\n",
      "Epoch 5/25\n",
      "34/34 [==============================] - ETA: 0s - loss: 1.3268 - categorical_accuracy: 0.4729\n",
      "Epoch 00005: saving model to model_init_2022-05-1512_21_22.342108/model-00005-1.32685-0.47285-1.36680-0.42000.h5\n",
      "34/34 [==============================] - 83s 3s/step - loss: 1.3268 - categorical_accuracy: 0.4729 - val_loss: 1.3668 - val_categorical_accuracy: 0.4200 - lr: 1.0000e-05\n",
      "Epoch 6/25\n",
      "34/34 [==============================] - ETA: 0s - loss: 1.2482 - categorical_accuracy: 0.5158\n",
      "Epoch 00006: saving model to model_init_2022-05-1512_21_22.342108/model-00006-1.24816-0.51584-1.31317-0.46000.h5\n",
      "34/34 [==============================] - 84s 3s/step - loss: 1.2482 - categorical_accuracy: 0.5158 - val_loss: 1.3132 - val_categorical_accuracy: 0.4600 - lr: 1.0000e-05\n",
      "Epoch 7/25\n",
      "34/34 [==============================] - ETA: 0s - loss: 1.1872 - categorical_accuracy: 0.5633\n",
      "Epoch 00007: saving model to model_init_2022-05-1512_21_22.342108/model-00007-1.18721-0.56335-1.23044-0.55000.h5\n",
      "34/34 [==============================] - 86s 3s/step - loss: 1.1872 - categorical_accuracy: 0.5633 - val_loss: 1.2304 - val_categorical_accuracy: 0.5500 - lr: 1.0000e-05\n",
      "Epoch 8/25\n",
      "34/34 [==============================] - ETA: 0s - loss: 1.0913 - categorical_accuracy: 0.6327\n",
      "Epoch 00008: saving model to model_init_2022-05-1512_21_22.342108/model-00008-1.09127-0.63273-1.24474-0.54000.h5\n",
      "34/34 [==============================] - 86s 3s/step - loss: 1.0913 - categorical_accuracy: 0.6327 - val_loss: 1.2447 - val_categorical_accuracy: 0.5400 - lr: 1.0000e-05\n",
      "Epoch 9/25\n",
      "34/34 [==============================] - ETA: 0s - loss: 1.0408 - categorical_accuracy: 0.6614\n",
      "Epoch 00009: saving model to model_init_2022-05-1512_21_22.342108/model-00009-1.04075-0.66139-1.18032-0.56000.h5\n",
      "34/34 [==============================] - 84s 3s/step - loss: 1.0408 - categorical_accuracy: 0.6614 - val_loss: 1.1803 - val_categorical_accuracy: 0.5600 - lr: 1.0000e-05\n",
      "Epoch 10/25\n",
      "34/34 [==============================] - ETA: 0s - loss: 0.9529 - categorical_accuracy: 0.7157\n",
      "Epoch 00010: saving model to model_init_2022-05-1512_21_22.342108/model-00010-0.95291-0.71569-1.04188-0.61000.h5\n",
      "34/34 [==============================] - 85s 3s/step - loss: 0.9529 - categorical_accuracy: 0.7157 - val_loss: 1.0419 - val_categorical_accuracy: 0.6100 - lr: 1.0000e-05\n",
      "Epoch 11/25\n",
      "34/34 [==============================] - ETA: 0s - loss: 0.8953 - categorical_accuracy: 0.7360\n",
      "Epoch 00011: saving model to model_init_2022-05-1512_21_22.342108/model-00011-0.89527-0.73605-1.02737-0.66000.h5\n",
      "34/34 [==============================] - 84s 3s/step - loss: 0.8953 - categorical_accuracy: 0.7360 - val_loss: 1.0274 - val_categorical_accuracy: 0.6600 - lr: 1.0000e-05\n",
      "Epoch 12/25\n",
      "34/34 [==============================] - ETA: 0s - loss: 0.8389 - categorical_accuracy: 0.7594\n",
      "Epoch 00012: saving model to model_init_2022-05-1512_21_22.342108/model-00012-0.83894-0.75943-1.00059-0.65000.h5\n",
      "34/34 [==============================] - 86s 3s/step - loss: 0.8389 - categorical_accuracy: 0.7594 - val_loss: 1.0006 - val_categorical_accuracy: 0.6500 - lr: 1.0000e-05\n",
      "Epoch 13/25\n",
      "34/34 [==============================] - ETA: 0s - loss: 0.7867 - categorical_accuracy: 0.7843\n",
      "Epoch 00013: saving model to model_init_2022-05-1512_21_22.342108/model-00013-0.78665-0.78431-0.92348-0.68000.h5\n",
      "34/34 [==============================] - 86s 3s/step - loss: 0.7867 - categorical_accuracy: 0.7843 - val_loss: 0.9235 - val_categorical_accuracy: 0.6800 - lr: 1.0000e-05\n",
      "Epoch 14/25\n",
      "34/34 [==============================] - ETA: 0s - loss: 0.7273 - categorical_accuracy: 0.8235\n",
      "Epoch 00014: saving model to model_init_2022-05-1512_21_22.342108/model-00014-0.72735-0.82353-0.89666-0.72000.h5\n",
      "34/34 [==============================] - 85s 3s/step - loss: 0.7273 - categorical_accuracy: 0.8235 - val_loss: 0.8967 - val_categorical_accuracy: 0.7200 - lr: 1.0000e-05\n",
      "Epoch 15/25\n",
      "34/34 [==============================] - ETA: 0s - loss: 0.6789 - categorical_accuracy: 0.8439\n",
      "Epoch 00015: saving model to model_init_2022-05-1512_21_22.342108/model-00015-0.67893-0.84389-0.90907-0.69000.h5\n",
      "34/34 [==============================] - 86s 3s/step - loss: 0.6789 - categorical_accuracy: 0.8439 - val_loss: 0.9091 - val_categorical_accuracy: 0.6900 - lr: 1.0000e-05\n",
      "Epoch 16/25\n",
      "34/34 [==============================] - ETA: 0s - loss: 0.6327 - categorical_accuracy: 0.8605\n",
      "Epoch 00016: saving model to model_init_2022-05-1512_21_22.342108/model-00016-0.63270-0.86048-0.83050-0.72000.h5\n",
      "34/34 [==============================] - 86s 3s/step - loss: 0.6327 - categorical_accuracy: 0.8605 - val_loss: 0.8305 - val_categorical_accuracy: 0.7200 - lr: 1.0000e-05\n",
      "Epoch 17/25\n",
      "34/34 [==============================] - ETA: 0s - loss: 0.5786 - categorical_accuracy: 0.8839\n",
      "Epoch 00017: saving model to model_init_2022-05-1512_21_22.342108/model-00017-0.57855-0.88386-0.82955-0.74000.h5\n",
      "34/34 [==============================] - 87s 3s/step - loss: 0.5786 - categorical_accuracy: 0.8839 - val_loss: 0.8296 - val_categorical_accuracy: 0.7400 - lr: 1.0000e-05\n",
      "Epoch 18/25\n",
      "34/34 [==============================] - ETA: 0s - loss: 0.5531 - categorical_accuracy: 0.8861\n",
      "Epoch 00018: saving model to model_init_2022-05-1512_21_22.342108/model-00018-0.55310-0.88612-0.81007-0.77000.h5\n",
      "34/34 [==============================] - 86s 3s/step - loss: 0.5531 - categorical_accuracy: 0.8861 - val_loss: 0.8101 - val_categorical_accuracy: 0.7700 - lr: 1.0000e-05\n",
      "Epoch 19/25\n",
      "34/34 [==============================] - ETA: 0s - loss: 0.5118 - categorical_accuracy: 0.8952\n",
      "Epoch 00019: saving model to model_init_2022-05-1512_21_22.342108/model-00019-0.51175-0.89517-0.76609-0.75000.h5\n",
      "34/34 [==============================] - 86s 3s/step - loss: 0.5118 - categorical_accuracy: 0.8952 - val_loss: 0.7661 - val_categorical_accuracy: 0.7500 - lr: 1.0000e-05\n",
      "Epoch 20/25\n",
      "34/34 [==============================] - ETA: 0s - loss: 0.4827 - categorical_accuracy: 0.9005\n",
      "Epoch 00020: saving model to model_init_2022-05-1512_21_22.342108/model-00020-0.48268-0.90045-0.69144-0.79000.h5\n",
      "34/34 [==============================] - 87s 3s/step - loss: 0.4827 - categorical_accuracy: 0.9005 - val_loss: 0.6914 - val_categorical_accuracy: 0.7900 - lr: 1.0000e-05\n",
      "Epoch 21/25\n",
      "34/34 [==============================] - ETA: 0s - loss: 0.4720 - categorical_accuracy: 0.8989\n",
      "Epoch 00021: saving model to model_init_2022-05-1512_21_22.342108/model-00021-0.47197-0.89894-0.69587-0.79000.h5\n",
      "34/34 [==============================] - 86s 3s/step - loss: 0.4720 - categorical_accuracy: 0.8989 - val_loss: 0.6959 - val_categorical_accuracy: 0.7900 - lr: 1.0000e-05\n",
      "Epoch 22/25\n",
      "34/34 [==============================] - ETA: 0s - loss: 0.4241 - categorical_accuracy: 0.9216\n",
      "Epoch 00022: saving model to model_init_2022-05-1512_21_22.342108/model-00022-0.42406-0.92157-0.73958-0.75000.h5\n",
      "34/34 [==============================] - 84s 3s/step - loss: 0.4241 - categorical_accuracy: 0.9216 - val_loss: 0.7396 - val_categorical_accuracy: 0.7500 - lr: 1.0000e-05\n",
      "Epoch 23/25\n",
      "34/34 [==============================] - ETA: 0s - loss: 0.4014 - categorical_accuracy: 0.9276\n",
      "Epoch 00023: saving model to model_init_2022-05-1512_21_22.342108/model-00023-0.40144-0.92760-0.66759-0.80000.h5\n",
      "34/34 [==============================] - 84s 3s/step - loss: 0.4014 - categorical_accuracy: 0.9276 - val_loss: 0.6676 - val_categorical_accuracy: 0.8000 - lr: 1.0000e-05\n",
      "Epoch 24/25\n",
      "34/34 [==============================] - ETA: 0s - loss: 0.3844 - categorical_accuracy: 0.9284\n",
      "Epoch 00024: saving model to model_init_2022-05-1512_21_22.342108/model-00024-0.38440-0.92836-0.58849-0.79000.h5\n",
      "34/34 [==============================] - 85s 3s/step - loss: 0.3844 - categorical_accuracy: 0.9284 - val_loss: 0.5885 - val_categorical_accuracy: 0.7900 - lr: 1.0000e-05\n",
      "Epoch 25/25\n",
      "34/34 [==============================] - ETA: 0s - loss: 0.3409 - categorical_accuracy: 0.9397\n",
      "Epoch 00025: saving model to model_init_2022-05-1512_21_22.342108/model-00025-0.34091-0.93967-0.58959-0.80000.h5\n",
      "34/34 [==============================] - 84s 3s/step - loss: 0.3409 - categorical_accuracy: 0.9397 - val_loss: 0.5896 - val_categorical_accuracy: 0.8000 - lr: 1.0000e-05\n"
     ]
    }
   ],
   "source": [
    "print(\"Total Params:\", rnn_cnn_tl2_model.count_params())\n",
    "history_model19=rnn_cnn_tl2.train_model(rnn_cnn_tl2_model,augment_data=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 576x576 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "acc = history.history['categorical_accuracy']\n",
    "val_acc = history.history['val_categorical_accuracy']\n",
    "\n",
    "loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']\n",
    "\n",
    "epochs_range = range(num_epochs)\n",
    "\n",
    "plt.figure(figsize=(8, 8))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(epochs_range, acc, label='Training Accuracy')\n",
    "plt.plot(epochs_range, val_acc, label='Validation Accuracy')\n",
    "plt.legend(loc='lower right')\n",
    "plt.title('Training and Validation Accuracy')\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(epochs_range, loss, label='Training Loss')\n",
    "plt.plot(epochs_range, val_loss, label='Validation Loss')\n",
    "plt.legend(loc='upper right')\n",
    "plt.title('Training and Validation Loss')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Conclusion: \n",
    "- <p>This model looks good with validation dataset with <i>validation accuracy</i> of <b>80%</b> and <i>training accuarcy</i> <b>93.97%</b>\n",
    "- ConvGRU based models have lot more parameters but outperformed when compared to conv3D based models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
